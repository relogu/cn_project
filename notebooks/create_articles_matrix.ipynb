{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fc578c",
   "metadata": {},
   "source": [
    "# Create articles matrix\n",
    "\n",
    "This notebook will create the article matrix by processing the .json returned from searching the articles of interest.\n",
    "The article matrix is thus composed of a matrix that has one line for every article and 300 columns representing the component of the vector of the article built with a Word2Vect model that has a 300-components representation for every word.\n",
    "Every article will be represented by the sum of the 300-components vector of its words weighted by the tf-idf score.\n",
    "The model chosen for the representation is the [Google News Word2Vec](https://code.google.com/archive/p/word2vec/) model.\n",
    "A dataframe containing the important features (`article_id`, `publish_time`, `source`) for the dynamical model is also returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74259d88-c6a4-4544-9edd-2bc014a9b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "from gensim.models.word2vec import KeyedVectors\n",
    "\n",
    "## These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be42dce",
   "metadata": {},
   "source": [
    "Function for processing the info of the articles from the .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00f5243-d4d8-49eb-b230-7923df008aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_info(story):\n",
    "    return pd.DataFrame(\n",
    "            [{\n",
    "                'article_id': story['stories_id'],\n",
    "                'timestamp': str(story['publish_date']),\n",
    "                'source': story['media_id'],\n",
    "            }]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb0adb",
   "metadata": {},
   "source": [
    "Function for processing the occurences of words of the articles from the .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3a2838-e3fd-4211-8ebc-c15c96b6792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article_word_matrix_json(article_words_occurences, n_words: int):\n",
    "    words_occurences = np.zeros(n_words)\n",
    "    for key, value in article_words_occurences.items():\n",
    "        words_occurences[eval(key)] = value\n",
    "    return words_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0e96c",
   "metadata": {},
   "source": [
    "Function for filtering the words, it turns out to be uncessary since the Word2Vect model does not give any representation to those words that were excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813d0199-af86-4a63-8c22-d816a40e0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = \".\\!@#$%^&*()+?_=,<>/\"\n",
    "\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def has_special_chars(inputString):\n",
    "    return any(c in special_characters for c in inputString)\n",
    "\n",
    "def process_drop_columns(col):\n",
    "    # tmp = pd.to_numeric(words_df[col])\n",
    "    # if len(col) < MIN_LENGTH or len(tmp[tmp>0]) == 0 or has_numbers(col) or has_special_chars(col):\n",
    "    if has_numbers(col) or has_special_chars(col):\n",
    "        return col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785b98c-ccf6-445d-8ae0-a853f1e84f4a",
   "metadata": {},
   "source": [
    "Load Word2Vect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd7b28-3118-4d68-aa31-2fa5a81d772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news_word2vec = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'word2vec-google-news-300.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aa8c3",
   "metadata": {},
   "source": [
    "Get the .json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6beea514-55a7-44aa-ab43-ca3ca8e64090",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'all_stories.json',\n",
    "    'word_matrix.json',\n",
    "    'np_docvs_norm.npz',\n",
    "    'dists_triu.csv',\n",
    "    'info_df.csv',\n",
    "    ]\n",
    "stories = [\n",
    "    'world_russia',\n",
    "    'world_norway',\n",
    "    'world_capitol_hill',\n",
    "]\n",
    "story_to_elaborate = 1\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[0]) as json_file:\n",
    "    all_stories = json.load(json_file)\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[1]) as json_file:\n",
    "    stories_words = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae4a14",
   "metadata": {},
   "source": [
    "Get the matrix of words occurences in articles, first step before computing the articles matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea95e663-1bc6-4103-a08a-d8955f16053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3f8e832ba64475b7bbd3e629cee245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2599 [00:00<?, ?articles/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_words = len(stories_words['word_list'])\n",
    "\n",
    "articles_iter = tqdm(\n",
    "    stories_words['word_matrix'].values(),\n",
    "    leave=True,\n",
    "    unit='articles',\n",
    ")\n",
    "fn = partial(process_article_word_matrix_json, n_words=n_words)\n",
    "results = np.array(Parallel(n_jobs=N_THREADS)(delayed(fn)(i) for i in articles_iter))\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044fa2d",
   "metadata": {},
   "source": [
    "Load and filter the model to be personalized to the actual vocabolary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52020b9d-6604-4d18-bb89-e5ae60ee039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [stories_words['word_list'][i][0] for i in range(len(stories_words['word_list']))]\n",
    "google_news_word2vec = google_news_word2vec.vectors_for_all(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b4b31",
   "metadata": {},
   "source": [
    "Function for getting the actual articles matrix.\n",
    "Every step of this process is optimized and parallelize to guarantee the maximum speed in processing exploiting the entire CPU capabilities (for `N_THREADS`=-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fea2d6b-89b9-48a2-ae2e-0d9bcc0d5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_articles_matrix(articles_words, model):\n",
    "    # total number of articles\n",
    "    n_articles = articles_words.shape[0]\n",
    "    # number of articles containing that word for every word\n",
    "    art_per_word = np.array([np.sum(articles_words[:,i]>0) for i in range(articles_words.shape[1])])\n",
    "    # all words idf\n",
    "    words_idf = np.log(n_articles/art_per_word)\n",
    "    \n",
    "    ## get all words vectors\n",
    "    words_iterator = tqdm(\n",
    "        all_words,\n",
    "        leave=True,\n",
    "        unit='words',\n",
    "    )\n",
    "    # function to parallelize \n",
    "    def get_word_vector(word):\n",
    "        try:\n",
    "            word_vector = model.get_vector(word)\n",
    "            try:\n",
    "                assert np.isfinite(word_vector).all()\n",
    "            except AssertionError:\n",
    "                print(word_vector)\n",
    "        except KeyError:\n",
    "            word_vector = [0]*300\n",
    "        return np.array(word_vector)\n",
    "    words_vectors = np.array([get_word_vector(word) for word in words_iterator])\n",
    "\n",
    "    ## get the articles vectors\n",
    "    # instatiate the article iterator\n",
    "    articles_iterator = tqdm(\n",
    "        articles_words,\n",
    "        leave=True,\n",
    "        unit='articles',\n",
    "    )\n",
    "    # function to parallelize \n",
    "    def get_article_vector(article):\n",
    "        article_vector = np.zeros((1, 300))\n",
    "        for i, word_vector in enumerate(words_vectors):\n",
    "            ## using tf-idf as weight\n",
    "            # occurences of word in the article\n",
    "            tf = article[i]\n",
    "            # if there are some\n",
    "            if tf > 0:\n",
    "                # tf-idf of word in article\n",
    "                weight = tf*words_idf[i]\n",
    "                # add with weight this word vector to whole article vector\n",
    "                article_vector = article_vector + word_vector*weight\n",
    "        return article_vector\n",
    "    list_of_docvs = Parallel(n_jobs=N_THREADS)(delayed(get_article_vector)(i) for i in articles_iterator)\n",
    "    \n",
    "    return np.array(list_of_docvs).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4f6de",
   "metadata": {},
   "source": [
    "Get the actual articles matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f413e299-4dc7-4eed-833c-79e3b9cdd594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02c4c0b7c9d49ecbcde30df7aa71d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188697 [00:00<?, ?words/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656ea197880044acbeecfd3fd07fd029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2599 [00:00<?, ?articles/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2599, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_matrix = create_articles_matrix(np.array(results), google_news_word2vec)\n",
    "articles_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a838354",
   "metadata": {},
   "source": [
    "Get the dataframe with the important features of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0835134b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30b7856fc344c588dca9930782c7493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2611 [00:00<?, ?stories/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>article_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>1996434927</td>\n",
       "      <td>2021-07-21 00:00:00</td>\n",
       "      <td>396984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1996327005</td>\n",
       "      <td>2021-07-21 00:00:00</td>\n",
       "      <td>125334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1996330114</td>\n",
       "      <td>2021-07-21 00:00:00</td>\n",
       "      <td>69934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>1996453424</td>\n",
       "      <td>2021-07-21 00:00:00</td>\n",
       "      <td>40268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1996202810</td>\n",
       "      <td>2021-07-21 00:19:04</td>\n",
       "      <td>84097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  article_id           timestamp  source\n",
       "52      0  1996434927 2021-07-21 00:00:00  396984\n",
       "2       0  1996327005 2021-07-21 00:00:00  125334\n",
       "3       0  1996330114 2021-07-21 00:00:00   69934\n",
       "58      0  1996453424 2021-07-21 00:00:00   40268\n",
       "1       0  1996202810 2021-07-21 00:19:04   84097"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_info_iter = tqdm(\n",
    "    all_stories,\n",
    "    leave=True,\n",
    "    unit='stories',\n",
    ")\n",
    "\n",
    "articles_info_df = pd.concat(Parallel(n_jobs=N_THREADS)(delayed(process_info)(i) for i in articles_info_iter), axis=0).reset_index()\n",
    "articles_info_df['timestamp'] = pd.to_datetime(articles_info_df.timestamp)\n",
    "articles_info_df = articles_info_df.sort_values(by='timestamp')\n",
    "articles_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66898b26",
   "metadata": {},
   "source": [
    "Check the articles retrieved in the two .json to be the same.\n",
    "The two sets of articles (the one from `*_all_stories.json`, and the one from `*_word_matrix.json`) will be filtered to contain the same articles in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca6acd",
   "metadata": {},
   "source": [
    "Ids for articles from `*_word_matrix.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c536dfa-449c-494d-ab64-735cca0cfbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2599,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_ids_word_matrix = np.array([eval(a) for a in list(stories_words['word_matrix'].keys())])\n",
    "articles_ids_word_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff4aff",
   "metadata": {},
   "source": [
    "Length of ids for articles from `*_all_stories.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9ad8e95-9f5a-4a60-890e-a49fd5587d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_info_df['article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3fa2e",
   "metadata": {},
   "source": [
    "Length of ids for articles from `*_all_stories.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48049734-92d9-4787-83b0-b90b350cf8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2599"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_ids_word_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6de689",
   "metadata": {},
   "source": [
    "Ids to be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ec00cfe-1ce1-4f2b-adb5-3ad1986ac0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2006074272,\n",
       " 2008001829,\n",
       " 2005206662,\n",
       " 2003482725,\n",
       " 2002532521,\n",
       " 2019848970,\n",
       " 2012380297,\n",
       " 2007440684,\n",
       " 2006087149,\n",
       " 2001692657,\n",
       " 1997648210,\n",
       " 1999354035,\n",
       " 2005497172,\n",
       " 1997714996,\n",
       " 2015712887,\n",
       " 2023509146,\n",
       " 2011491326,\n",
       " 2021793311]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_from_df = list(set(articles_info_df['article_id']) - set(articles_ids_word_matrix))\n",
    "remove_from_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3210d0",
   "metadata": {},
   "source": [
    "Ids to be removed from matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01ec29a5-1be6-4d36-bbff-b0fac7498698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2001486402, 2007730531, 2015181549, 1997922062, 2132215122, 2021374618]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_from_matrix = list(set(articles_ids_word_matrix) - set(articles_info_df['article_id']))\n",
    "remove_from_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabc5a6",
   "metadata": {},
   "source": [
    "Deletion from dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c5575c5-dd7d-4beb-bc4a-7a61f610778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2593"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[articles_info_df.drop(articles_info_df[articles_info_df['article_id'] == i].index, inplace=True) for i in remove_from_df]\n",
    "len(articles_info_df['article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510e0a2",
   "metadata": {},
   "source": [
    "Deletion from matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee5f6a02-a399-4b38-b9dc-bf98a76f0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2593, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [articles_ids_word_matrix.tolist().index(i) for i in remove_from_matrix]\n",
    "articles_ids_word_matrix = np.delete(articles_ids_word_matrix, indices, axis=0)\n",
    "articles_matrix = np.delete(articles_matrix, indices, axis=0)\n",
    "articles_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1120b",
   "metadata": {},
   "source": [
    "Reordering of articles in the matrix, since we want them to be in the same order (ascending in time) of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c6b4ffb-a576-4c21-aede-9b6d4ed8b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices = [articles_ids_word_matrix.tolist().index(i) for i in articles_info_df['article_id']]\n",
    "new_articles_matrix = articles_matrix.copy()\n",
    "for i,j in enumerate(new_indices):\n",
    "    new_articles_matrix[i,:] = new_articles_matrix[j,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f2abc",
   "metadata": {},
   "source": [
    "Normalize and save the article matrix using numpy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a538dd3d-d141-4b56-b40a-79161ab9c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of normalized matrix is (2593, 300).\n",
      "Sum of normalized matrix is -3708.213049179245.\n",
      "Max=0.24483098695949793; Min=-0.21940790958173134.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2593, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = articles_matrix.sum(axis=1)\n",
    "np_docvs_norm = (articles_matrix / np.sqrt((articles_matrix ** 2).sum(-1))[..., np.newaxis]).astype('float')\n",
    "print(\"Shape of normalized matrix is {}.\".format(np_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(np_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(np_docvs_norm), np.min(np_docvs_norm)))\n",
    "np.savez(PATH_TO_DATA/stories[story_to_elaborate]/files[2], np_docvs_norm)\n",
    "np_docvs_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be407356",
   "metadata": {},
   "source": [
    "Compute and save up-triangular distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f906042e-0db9-4195-b8f6-728bbf1c4268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of similarity matrix is (2593, 2593).\n",
      "Sum of similarity matrix is 2402656.978175076.\n",
      "Max=1.0000000000000007; Min=0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.74970341, 0.8454205 , ..., 0.87679063, 0.80614634,\n",
       "        0.83195407],\n",
       "       [0.        , 0.        , 0.76036679, ..., 0.78907547, 0.71995947,\n",
       "        0.68784944],\n",
       "       [0.        , 0.        , 0.        , ..., 0.84411785, 0.75879691,\n",
       "        0.74960482],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.83620062,\n",
       "        0.86417294],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.87251212],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.dot(np_docvs_norm, np_docvs_norm.T).astype('float')\n",
    "dists_triu = np.triu(dists, k=1)\n",
    "np.savetxt(PATH_TO_DATA/stories[story_to_elaborate]/files[3], dists_triu, delimiter=',')\n",
    "print(\"Shape of similarity matrix is {}.\".format(dists_triu.shape))\n",
    "print(\"Sum of similarity matrix is {}.\".format(np.sum(dists_triu)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(dists_triu), np.min(dists_triu)))\n",
    "dists_triu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a681a6",
   "metadata": {},
   "source": [
    "Save the important features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "983b4828-b5d0-46a3-ad12-f4d25045e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[4], 'w') as csv_file:\n",
    "    articles_info_df.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907cfb8-3313-4906-bd82-f61e47157910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
