{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74259d88-c6a4-4544-9edd-2bc014a9b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import mediacloud.api\n",
    "from IPython.display import JSON\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from gensim.models.word2vec import KeyedVectors\n",
    "\n",
    "## These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00f5243-d4d8-49eb-b230-7923df008aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_info(story):\n",
    "    return pd.DataFrame(\n",
    "            [{\n",
    "                'article_id': story['stories_id'],\n",
    "                'timestamp': str(story['publish_date']),\n",
    "                'source': story['media_id'],\n",
    "            }]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3a2838-e3fd-4211-8ebc-c15c96b6792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article_word_matrix_json(article_words_occurences, n_words: int):\n",
    "    words_occurences = np.zeros(n_words)\n",
    "    for key, value in article_words_occurences.items():\n",
    "        words_occurences[eval(key)] = value\n",
    "    return words_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813d0199-af86-4a63-8c22-d816a40e0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = \".\\!@#$%^&*()+?_=,<>/\"\n",
    "\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def has_special_chars(inputString):\n",
    "    return any(c in special_characters for c in inputString)\n",
    "\n",
    "def process_drop_columns(col):\n",
    "    # tmp = pd.to_numeric(words_df[col])\n",
    "    # if len(col) < MIN_LENGTH or len(tmp[tmp>0]) == 0 or has_numbers(col) or has_special_chars(col):\n",
    "    if has_numbers(col) or has_special_chars(col):\n",
    "        return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4548dbe5-ded3-403b-a87c-353684ac9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ids(id, articles_ids):\n",
    "    if id in articles_ids:\n",
    "        return None\n",
    "    else:\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6beea514-55a7-44aa-ab43-ca3ca8e64090",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = {\n",
    "    'norway_attack': ['world_norway_all_stories.json', 'world_norway_word_matrix.json'],\n",
    "    'russia_shooting': ['world_russia_all_stories.json', 'world_russia_word_matrix.json'],\n",
    "    'capitol_hill': ['world_capitol_hill_all_stories.json', 'world_capitol_hill_word_matrix.json'],\n",
    "}\n",
    "story_to_elaborate = 'russia_shooting' # 'russia_shooting' # 'capitol_hill'\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate][0]) as json_file:\n",
    "    all_stories = json.load(json_file)\n",
    "# JSON(all_stories)\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate][1]) as json_file:\n",
    "    stories_words = json.load(json_file)\n",
    "# JSON(stories_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea95e663-1bc6-4103-a08a-d8955f16053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584334a80064566b0ed5f012626ecd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3330 [00:00<?, ?articles/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_words = len(stories_words['word_list'])\n",
    "\n",
    "articles_iter = tqdm(\n",
    "    stories_words['word_matrix'].values(),\n",
    "    leave=True,\n",
    "    unit='articles',\n",
    ")\n",
    "fn = partial(process_article_word_matrix_json, n_words=n_words)\n",
    "results = np.array(Parallel(n_jobs=N_THREADS)(delayed(fn)(i) for i in articles_iter))\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52020b9d-6604-4d18-bb89-e5ae60ee039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [stories_words['word_list'][i][0] for i in range(len(stories_words['word_list']))]\n",
    "google_news_word2vec = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'word2vec-google-news-300.gz', binary=True)\n",
    "google_news_word2vec = google_news_word2vec.vectors_for_all(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fea2d6b-89b9-48a2-ae2e-0d9bcc0d5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_articles_matrix(articles_words, model):\n",
    "    # total number of articles\n",
    "    n_articles = articles_words.shape[0]\n",
    "    # number of articles containing that word for every word\n",
    "    art_per_word = np.array([np.sum(articles_words[:,i]>0) for i in range(articles_words.shape[1])])\n",
    "    # all words idf\n",
    "    words_idf = np.log(n_articles/art_per_word)\n",
    "    \n",
    "    ## get all words vectors\n",
    "    words_iterator = tqdm(\n",
    "        all_words,\n",
    "        leave=True,\n",
    "        unit='words',\n",
    "    )\n",
    "    # function to parallelize \n",
    "    def get_word_vector(word):\n",
    "        try:\n",
    "            word_vector = google_news_word2vec.get_vector(word)\n",
    "            try:\n",
    "                assert np.isfinite(word_vector).all()\n",
    "            except AssertionError:\n",
    "                print(word_vector)\n",
    "        except KeyError:\n",
    "            word_vector = [0]*300\n",
    "        return np.array(word_vector)\n",
    "    words_vectors = np.array([get_word_vector(word) for word in words_iterator])\n",
    "\n",
    "    ## get the articles vectors\n",
    "    # instatiate the article iterator\n",
    "    articles_iterator = tqdm(\n",
    "        articles_words,\n",
    "        leave=True,\n",
    "        unit='articles',\n",
    "    )\n",
    "    # function to parallelize \n",
    "    def get_article_vector(article):\n",
    "        article_vector = np.zeros((1, 300))\n",
    "        for i, word_vector in enumerate(words_vectors):\n",
    "            ## using tf-idf as weight\n",
    "            # occurences of word in the article\n",
    "            tf = article[i]\n",
    "            # if there are some\n",
    "            if tf > 0:\n",
    "                # tf-idf of word in article\n",
    "                weight = tf*words_idf[i]\n",
    "                # add with weight this word vector to whole article vector\n",
    "                article_vector = article_vector + word_vector*weight\n",
    "        return article_vector\n",
    "    list_of_docvs = Parallel(n_jobs=N_THREADS)(delayed(get_article_vector)(i) for i in articles_iterator)\n",
    "    \n",
    "    return np.array(list_of_docvs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f413e299-4dc7-4eed-833c-79e3b9cdd594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f56c162f294c04be7fb3e1117fc08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191795 [00:00<?, ?words/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96427cc96fa34b169a62163626406462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3330 [00:00<?, ?articles/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_matrix = create_articles_matrix(np.array(results), google_news_word2vec)\n",
    "articles_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c536dfa-449c-494d-ab64-735cca0cfbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3330,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_ids_word_matrix = np.array([eval(a) for a in list(stories_words['word_matrix'].keys())])\n",
    "articles_ids_word_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2dff7e2-4cbb-4bbd-abd9-217131f77830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb875635fed4390b7b6572c99fa2b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3332 [00:00<?, ?stories/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>article_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1926391701</td>\n",
       "      <td>2021-05-10 00:00:00</td>\n",
       "      <td>278722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1926430980</td>\n",
       "      <td>2021-05-10 00:00:00</td>\n",
       "      <td>20270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1926386660</td>\n",
       "      <td>2021-05-10 00:01:35</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1926388426</td>\n",
       "      <td>2021-05-10 00:01:35</td>\n",
       "      <td>63091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1926416043</td>\n",
       "      <td>2021-05-10 00:58:55</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  article_id           timestamp  source\n",
       "0      0  1926391701 2021-05-10 00:00:00  278722\n",
       "4      0  1926430980 2021-05-10 00:00:00   20270\n",
       "1      0  1926386660 2021-05-10 00:01:35    1123\n",
       "2      0  1926388426 2021-05-10 00:01:35   63091\n",
       "3      0  1926416043 2021-05-10 00:58:55    1123"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_info_iter = tqdm(\n",
    "    all_stories,\n",
    "    leave=True,\n",
    "    unit='stories',\n",
    ")\n",
    "\n",
    "articles_info_df = pd.concat(Parallel(n_jobs=N_THREADS)(delayed(process_info)(i) for i in articles_info_iter), axis=0).reset_index()\n",
    "articles_info_df['timestamp'] = pd.to_datetime(articles_info_df.timestamp)\n",
    "articles_info_df = articles_info_df.sort_values(by='timestamp')\n",
    "articles_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ad8e95-9f5a-4a60-890e-a49fd5587d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3332"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_info_df['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48049734-92d9-4787-83b0-b90b350cf8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_ids_word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec00cfe-1ce1-4f2b-adb5-3ad1986ac0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1928891878,\n",
       " 1927618664,\n",
       " 1950003755,\n",
       " 1956817933,\n",
       " 1927927920,\n",
       " 1952064209,\n",
       " 1928082225,\n",
       " 1928836306,\n",
       " 1940974713]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_from_df = list(set(articles_info_df['article_id']) - set(articles_ids_word_matrix))\n",
    "remove_from_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ec29a5-1be6-4d36-bbff-b0fac7498698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1928431232,\n",
       " 1927686400,\n",
       " 1951160778,\n",
       " 2139135120,\n",
       " 1930100244,\n",
       " 1927900885,\n",
       " 1936130399]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_from_matrix = list(set(articles_ids_word_matrix) - set(articles_info_df['article_id']))\n",
    "remove_from_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c5575c5-dd7d-4beb-bc4a-7a61f610778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3323"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[articles_info_df.drop(articles_info_df[articles_info_df['article_id'] == i].index, inplace=True) for i in remove_from_df]\n",
    "len(articles_info_df['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee5f6a02-a399-4b38-b9dc-bf98a76f0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3323, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [articles_ids_word_matrix.tolist().index(i) for i in remove_from_matrix]\n",
    "articles_ids_word_matrix = np.delete(articles_ids_word_matrix, indices, axis=0)\n",
    "articles_matrix = np.delete(articles_matrix, indices, axis=0)\n",
    "articles_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab30c6f8-1e70-4d58-a4ee-335e6da22aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([388.13684672,  56.24779515,  32.1376547 , 539.17715772])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_matrix[0,:].shape\n",
    "articles_matrix[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c6b4ffb-a576-4c21-aede-9b6d4ed8b3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102.72295375, 305.62167091, -84.02320447, 499.11175712])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_indices = [articles_ids_word_matrix.tolist().index(i) for i in articles_info_df['article_id']]\n",
    "new_articles_matrix = articles_matrix.copy()\n",
    "for i,j in enumerate(new_indices):\n",
    "    new_articles_matrix[i,:] = new_articles_matrix[j,:]\n",
    "new_articles_matrix[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a538dd3d-d141-4b56-b40a-79161ab9c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of normalized matrix is (3323, 300).\n",
      "Sum of normalized matrix is -4756.11382784786.\n",
      "Max=0.2640060402203688; Min=-0.22912924849362137.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3323, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy normalization procedure\n",
    "row_sums = articles_matrix.sum(axis=1)\n",
    "np_docvs_norm = (articles_matrix / np.sqrt((articles_matrix ** 2).sum(-1))[..., np.newaxis]).astype('float')\n",
    "print(\"Shape of normalized matrix is {}.\".format(np_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(np_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(np_docvs_norm), np.min(np_docvs_norm)))\n",
    "np.savez(PATH_TO_DATA/str(story_to_elaborate+'_np_docvs_norm.npz'), np_docvs_norm)\n",
    "np_docvs_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f906042e-0db9-4195-b8f6-728bbf1c4268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of similarity matrix is (3323, 3323).\n",
      "Sum of similarity matrix is 3977099.632397247.\n",
      "Max=1.0000000000000009; Min=0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.83808093, ..., 0.84662232, 0.84437757,\n",
       "        0.903637  ],\n",
       "       [0.        , 0.        , 0.83808093, ..., 0.84662232, 0.84437757,\n",
       "        0.903637  ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.84400214, 0.8458315 ,\n",
       "        0.86671762],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.99031549,\n",
       "        0.96613961],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.96762526],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.dot(np_docvs_norm, np_docvs_norm.T).astype('float')\n",
    "dists_triu = np.triu(dists, k=1)\n",
    "np.savetxt(PATH_TO_DATA/str(story_to_elaborate+'_dists_triu.csv'), dists_triu, delimiter=',')\n",
    "print(\"Shape of similarity matrix is {}.\".format(dists_triu.shape))\n",
    "print(\"Sum of similarity matrix is {}.\".format(np.sum(dists_triu)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(dists_triu), np.min(dists_triu)))\n",
    "dists_triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "983b4828-b5d0-46a3-ad12-f4d25045e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA/str(story_to_elaborate+'_info_df.csv'), 'w') as csv_file:\n",
    "    articles_info_df.to_csv(csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
