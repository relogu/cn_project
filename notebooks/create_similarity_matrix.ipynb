{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec5ae0",
   "metadata": {},
   "source": [
    "# Computing Similarity Matrix\n",
    "\n",
    "This notebook will compute the similarity matrix given the `word_matrix` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5747b823-cf56-4939-b9d5-8c86e64c94ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:30:02.566159Z",
     "start_time": "2022-01-08T16:30:02.205349Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from gensim.models.word2vec import KeyedVectors\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256eee93",
   "metadata": {},
   "source": [
    "Get the dataframe from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67729628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intanto</th>\n",
       "      <th>tarrio</th>\n",
       "      <th>votat</th>\n",
       "      <th>quando</th>\n",
       "      <th>è</th>\n",
       "      <th>banner</th>\n",
       "      <th>vota</th>\n",
       "      <th>dandolo</th>\n",
       "      <th>fiamm</th>\n",
       "      <th>si</th>\n",
       "      <th>...</th>\n",
       "      <th>l'oligarchia</th>\n",
       "      <th>dopò</th>\n",
       "      <th>gravano</th>\n",
       "      <th>sovrappongono</th>\n",
       "      <th>ascoltatori</th>\n",
       "      <th>sopravvivrà</th>\n",
       "      <th>paragonando</th>\n",
       "      <th>attaccavano</th>\n",
       "      <th>stento</th>\n",
       "      <th>un'audi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40021 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   intanto  tarrio  votat  quando    è  banner  vota  dandolo  fiamm   si  \\\n",
       "0      0.0     0.0    0.0     0.0  0.0     0.0   0.0      0.0    0.0  0.0   \n",
       "1      0.0     0.0    0.0     0.0  0.0     0.0   0.0      0.0    0.0  0.0   \n",
       "2      1.0     2.0    1.0     1.0  3.0     1.0   1.0      1.0    1.0  2.0   \n",
       "3      1.0     2.0    1.0     1.0  3.0     1.0   1.0      1.0    1.0  2.0   \n",
       "4      0.0     0.0    0.0     0.0  0.0     0.0   0.0      0.0    0.0  0.0   \n",
       "\n",
       "   ...  l'oligarchia  dopò  gravano  sovrappongono  ascoltatori  sopravvivrà  \\\n",
       "0  ...           0.0   0.0      0.0            0.0          0.0          0.0   \n",
       "1  ...           0.0   0.0      0.0            0.0          0.0          0.0   \n",
       "2  ...           0.0   0.0      0.0            0.0          0.0          0.0   \n",
       "3  ...           0.0   0.0      0.0            0.0          0.0          0.0   \n",
       "4  ...           0.0   0.0      0.0            0.0          0.0          0.0   \n",
       "\n",
       "   paragonando  attaccavano  stento  un'audi  \n",
       "0          0.0          0.0     0.0      0.0  \n",
       "1          0.0          0.0     0.0      0.0  \n",
       "2          0.0          0.0     0.0      0.0  \n",
       "3          0.0          0.0     0.0      0.0  \n",
       "4          0.0          0.0     0.0      0.0  \n",
       "\n",
       "[5 rows x 40021 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = {\n",
    "    'norway_attack': 'world_norway_word_matrix_df.csv',\n",
    "    'russia_shooting': 'world_russia_word_matrix_df.csv',\n",
    "    'capitol_hill': 'world_capitol_hill_word_matrix_df.csv',\n",
    "    'test': 'word_matrix_df.csv',\n",
    "}\n",
    "story_to_elaborate = 'test'# 'norway_attack' # 'russia_shooting' # 'capitol_hill'\n",
    "with open(PATH_TO_DATA/dataframes[story_to_elaborate]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    word_matrix_df = pd.read_csv(csv_file).drop(['Unnamed: 0','article_id'], 1)\n",
    "word_matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49565de-1bd6-44ef-aae8-55ad2dedd36c",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to create the **Similarity Matrix**, we should _calculate the distances_ of all the articles (exploiting the simple dot product) from the loaded dataframe.\n",
    "'WORD_VECTORS' must be passed through the `create_model_matrix` function that will map each document to its 300-dim vector representation using a model from `gensim`.\n",
    "This model can be dowloaded using the downloading API of `gensim`, more information can be found [here](https://github.com/RaRe-Technologies/gensim-data).\n",
    "The available models to choose are [ConceptNet Numberbatch](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972) or [Google News Word2Vec](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "The results of `create_model_matrix` must be normalized before computing the distance.\n",
    "The fastest method for normalization is that from _sklearn_, but also a _numpy_ version is provided (but not used).\n",
    "\n",
    "This is equivalent to use the _cosine similarity_ distance to compute these distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd44b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the possible words\n",
    "all_words = list(word_matrix_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e1599",
   "metadata": {},
   "source": [
    "Then, the function for creating the whole word matrix is defined, TFITF score is used as weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9f4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_matrix(data, model):\n",
    "\n",
    "    # instatiate the iterator\n",
    "    articles_iterator = tqdm(\n",
    "        range(len(data)),\n",
    "        leave=True,\n",
    "        unit='articles',\n",
    "    )\n",
    "\n",
    "    # function to parallelize \n",
    "    def fn(article):\n",
    "        article_vector = np.zeros((1, 300))\n",
    "        for i, word in enumerate(all_words):\n",
    "            # using tf-idf as weight\n",
    "            tf = data[article,i]\n",
    "            n = np.sum(data[:,i])\n",
    "            idtf = np.log(len(data)/n)\n",
    "            weight = tf*idtf\n",
    "            try:\n",
    "                word_vector = model.get_vector(word)\n",
    "                try:\n",
    "                    assert np.isfinite(word_vector).all()\n",
    "                except AssertionError:\n",
    "                    print(word_vector)\n",
    "            except KeyError:\n",
    "                word_vector = np.zeros((1, 300))\n",
    "            article_vector = article_vector + word_vector*weight\n",
    "        return article_vector\n",
    "            \n",
    "    list_of_docvs = Parallel(n_jobs=N_THREADS)(delayed(fn)(i) for i in articles_iterator)\n",
    "    return np.array(list_of_docvs).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51499d2f",
   "metadata": {},
   "source": [
    "Loading the [ConceptNet Numberbatch](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972) model, and adapting to our words.\n",
    "DONT'T USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptnet = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'conceptnet-numberbatch-17-06-300.gz')\n",
    "conceptnet = conceptnet.vectors_for_all(all_words)\n",
    "conceptnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f731e9",
   "metadata": {},
   "source": [
    "Loading the [Google News Word2Vec](https://code.google.com/archive/p/word2vec/) model, and adapting to our words.\n",
    "USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e17e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fc12252a7a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_word2vec = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'word2vec-google-news-300.gz', binary=True)\n",
    "google_news_word2vec = google_news_word2vec.vectors_for_all(all_words)\n",
    "google_news_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def882bb",
   "metadata": {},
   "source": [
    "Get the whole word matrix using the DataFrame and the model choosen.\n",
    "DONT'T USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conceptnet\n",
    "conceptnet_docvs = create_model_matrix(np.array(word_matrix_df), model)\n",
    "np.savez(PATH_TO_DATA/'conceptnet_docvs.npz', conceptnet_docvs)\n",
    "conceptnet_docvs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ff0e4",
   "metadata": {},
   "source": [
    "USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75ae32-242a-4e28-906d-f1424a1c01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = google_news_word2vec\n",
    "google_docvs = create_model_matrix(np.array(word_matrix_df), model)\n",
    "np.savez(PATH_TO_DATA/'google_docvs.npz', google_docvs)\n",
    "google_docvs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee17946-503a-4f99-bdfd-091d45c35283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.15124185e+00,  8.49358914e+00,  3.24921606e+00, ...,\n",
       "        -7.54261422e+00,  4.95666061e+00, -2.39626000e+00],\n",
       "       [ 2.14510083e+01,  3.21642291e+01,  5.47439096e+00, ...,\n",
       "        -3.59560750e+01,  1.89494194e+01,  8.06504266e-01],\n",
       "       [-2.01376798e+01,  5.91221505e+01,  1.30303238e+02, ...,\n",
       "        -5.36260845e+01, -7.60163316e+00,  1.79734167e+02],\n",
       "       ...,\n",
       "       [-1.39637447e+01,  2.76800045e+01, -9.91592173e-02, ...,\n",
       "        -2.79628069e+01, -1.12852966e+01,  1.77993376e+01],\n",
       "       [-4.35520784e+00,  5.69531574e+00,  6.75323993e+00, ...,\n",
       "        -6.08904045e+00,  2.29433093e+00,  1.00694237e+01],\n",
       "       [-8.89013666e+00,  6.28166851e+00, -5.81385224e+00, ...,\n",
       "        -4.23384561e+00, -3.51962592e+00, -5.14883912e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docvs = conceptnet_docvs\n",
    "docvs = google_docvs\n",
    "docvs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf6188",
   "metadata": {},
   "source": [
    "`numpy` normalization procedure.\n",
    "USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8253e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of normalized matrix is (2992, 300).\n",
      "Sum of normalized matrix is -2821.8530565127417.\n",
      "Max=0.26247709861429375; Min=-0.24015063773379133.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2992, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy normalization procedure\n",
    "row_sums = docvs.sum(axis=1)\n",
    "np_docvs_norm = (docvs / np.sqrt((docvs ** 2).sum(-1))[..., np.newaxis]).astype('float')\n",
    "print(\"Shape of normalized matrix is {}.\".format(np_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(np_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(np_docvs_norm), np.min(np_docvs_norm)))\n",
    "np.savez(PATH_TO_DATA/'np_docvs_norm.npz', np_docvs_norm)\n",
    "np_docvs_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000911c",
   "metadata": {},
   "source": [
    "`sklearn` normalization procedure (axis=1).\n",
    "DONT'T USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b28e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn normalization procedure (axis=1)\n",
    "sk_docvs_norm = normalize(docvs)\n",
    "print(\"Shape of normalized matrix is {}.\".format(sk_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(sk_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(sk_docvs_norm), np.min(sk_docvs_norm)))\n",
    "np.savez(PATH_TO_DATA/'sk_docvs_norm.npz', sk_docvs_norm)\n",
    "sk_docvs_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded9e46",
   "metadata": {},
   "source": [
    "`sklearn` normalization procedure (axis=0).\n",
    "DONT'T USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn normalization procedure (axis=0)\n",
    "sk_docvs_norm_0 = normalize(docvs, axis=0)\n",
    "print(\"Shape of normalized matrix is {}.\".format(sk_docvs_norm_0.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(sk_docvs_norm_0)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(sk_docvs_norm_0), np.min(sk_docvs_norm_0)))\n",
    "np.savez(PATH_TO_DATA/'sk_docvs_norm_0.npz', sk_docvs_norm_0)\n",
    "sk_docvs_norm_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712f297",
   "metadata": {},
   "source": [
    "Computation of the distance matrix.\n",
    "The simple dot product is used between the matrix and its transpose.\n",
    "Here are used `scipy.sparse` matrices.\n",
    "DONT'T USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee5180-2a9b-40bb-9105-843e378effe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sparse.csr_matrix(sk_docvs_norm_0)\n",
    "s_t = sparse.csr_matrix(sk_docvs_norm_0).T\n",
    "s_dist = s.dot(s_t)\n",
    "dists_triu = sparse.triu(s_dist, k=1)\n",
    "dists_triu = np.array(dists_triu.todense())\n",
    "np.savetxt(PATH_TO_DATA/'dists_triu.csv', dists_triu, delimiter=',')\n",
    "dists_triu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a5467",
   "metadata": {},
   "source": [
    "USE THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef580f77-00d7-45e0-bf22-09cc085e1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of similarity matrix is (2992, 2992).\n",
      "Sum of similarity matrix is 2725212.7520491164.\n",
      "Max=1.0000000000000004; Min=-0.4190984022529244.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.71497394, 0.50444967, ..., 0.66298975, 0.61015226,\n",
       "        0.65692472],\n",
       "       [0.        , 0.        , 0.46490303, ..., 0.64113891, 0.7083136 ,\n",
       "        0.63537908],\n",
       "       [0.        , 0.        , 0.        , ..., 0.57545389, 0.56454262,\n",
       "        0.46803413],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.65108241,\n",
       "        0.57432244],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.59058211],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.dot(np_docvs_norm, np_docvs_norm.T).astype('float')\n",
    "dists_triu = np.triu(dists, k=1)\n",
    "np.savetxt(PATH_TO_DATA/story_to_elaborate+'_dists_triu.csv', dists_triu, delimiter=',')\n",
    "print(\"Shape of similarity matrix is {}.\".format(dists_triu.shape))\n",
    "print(\"Sum of similarity matrix is {}.\".format(np.sum(dists_triu)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(dists_triu), np.min(dists_triu)))\n",
    "dists_triu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
