{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical data analysis\n",
    "\n",
    "This notebook will perform the necessary anlysis to find the best parameters for the creation of the empirical network.\n",
    "The parameter to be found are the `minimum similarity`, that will say whether an article has influenced another, and `maximum time distance` between tow similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "seaborn.set_style(\"whitegrid\")\n",
    "\n",
    "## These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# number of days of collection of articles\n",
    "N_DAYS = 30\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60242/2152746974.py:24: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  info_df = pd.read_csv(csv_file).drop(['Unnamed: 0'], 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  article_id           timestamp  source\n",
      "0      0  1926391701 2021-05-10 00:00:00  278722\n",
      "1      0  1926430980 2021-05-10 00:00:00   20270\n",
      "2      0  1926386660 2021-05-10 00:01:35    1123\n",
      "3      0  1926388426 2021-05-10 00:01:35   63091\n",
      "4      0  1926416043 2021-05-10 00:58:55    1123\n",
      "(3323, 3323)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    'all_stories.json',\n",
    "    'word_matrix.json',\n",
    "    'np_docvs_norm.npz',\n",
    "    'dists_triu.csv',\n",
    "    'info_df.csv',\n",
    "    ]\n",
    "imgs = [\n",
    "    '3Dhyperparameter_grid_search.png',\n",
    "    'time_influence_range.png',\n",
    "    'time_influence_range_violinplot.png',\n",
    "    'similarity_dist_pairwise.png',\n",
    "    'similarity_dist_most_similar.png',\n",
    "    '2Dhyperparameter_grid_search.png',\n",
    "    ]\n",
    "stories = [\n",
    "    'world_russia',\n",
    "    'world_norway',\n",
    "    'world_capitol_hill',\n",
    "]\n",
    "story_to_elaborate = 0\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[4]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    info_df = pd.read_csv(csv_file).drop(['Unnamed: 0'], 1)\n",
    "info_df = info_df.rename(columns={'id': 'article_id'})\n",
    "info_df['timestamp'] = pd.to_datetime(info_df.timestamp)\n",
    "print(info_df.head())\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[3]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    dists_triu = pd.read_csv(csv_file, sep=',', header=None)\n",
    "dists_triu = dists_triu.values\n",
    "print(dists_triu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3323"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ids = list(info_df['article_id'])\n",
    "len(list_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for building the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(data, timestamp, similarities, max_dt, min_similarity, outs):\n",
    "    while True:\n",
    "        # get the maximum similarity w.r.t. older articles\n",
    "        similarity = max(similarities)\n",
    "        # get the index of such max similar article\n",
    "        index = similarities.index(similarity)\n",
    "        # get dt in terms of hours\n",
    "        dt = (timestamp - data['timestamp'][index]).total_seconds() / 3600\n",
    "        # similarity threshold\n",
    "        if similarity < min_similarity:\n",
    "            # return None to add index to outs\n",
    "            return None\n",
    "        # continue if article is in outs or its distant in time\n",
    "        elif index in outs or dt > max_dt:\n",
    "            similarities[index] = 0\n",
    "        # pass condition\n",
    "        else:\n",
    "            return index\n",
    "\n",
    "def create_graph(\n",
    "    dists_triu, # similarity matrix\n",
    "    data, # DataFrame containing articles info\n",
    "    time_max: int = 168, # max dt (in hours) for two articles to be linked\n",
    "    sim_min: float = 0.8, # minimum cos sim distance for two articles to be linked\n",
    "    ):\n",
    "    # max number of nodes\n",
    "    n_articles = dists_triu.shape[0]\n",
    "    # instantiate the directed graph\n",
    "    G = nx.Graph()\n",
    "    # adding the first node\n",
    "    G.add_node(0, step=0,\n",
    "               date=data['timestamp'][0],\n",
    "               domain=data['source'][0],\n",
    "               _id=data['article_id'][0],\n",
    "               children=[])\n",
    "    # instatiating elimination list\n",
    "    outs = []\n",
    "    # loop on the other articles\n",
    "    for i in range(1, n_articles):\n",
    "        # get time of current article\n",
    "        pub_i = data['timestamp'][i]\n",
    "        # get similarities\n",
    "        column = list(dists_triu[:, i])\n",
    "        # get index of an article related to the current one\n",
    "        index = get_index(data, pub_i, column, time_max, sim_min, outs)\n",
    "        # if a relation was found\n",
    "        if index != None:\n",
    "            # if the related article has not already been inserted, insert it\n",
    "            if index not in G.nodes():\n",
    "                G.add_nodes_from([(index, {\n",
    "                    'timestamp': data['timestamp'][index],\n",
    "                    'source': data['source'][index],\n",
    "                    'id': data['article_id'][index],\n",
    "                })])\n",
    "            # if the current article has not already been inserted, insert it\n",
    "            if i not in G.nodes():\n",
    "                G.add_nodes_from([(i, {\n",
    "                    'timestamp': data['timestamp'][i],\n",
    "                    'source': data['source'][i],\n",
    "                    'id': data['article_id'][i]\n",
    "                })])\n",
    "            # linking the nodes\n",
    "            G.add_edge(index, i)\n",
    "        # if a relation wa not found\n",
    "        else:\n",
    "            # add current article to elimination listS\n",
    "            outs.append(i)\n",
    "    # return the graph\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting the giant componend of a created graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_nodes(\n",
    "    params\n",
    "):\n",
    "    G = create_graph(dists_triu, info_df, time_max=params[1], sim_min=params[0])\n",
    "    Gcc = sorted((G.subgraph(c) for c in nx.connected_components(G)), key = len, reverse=True)\n",
    "    G0 = Gcc[0]       \n",
    "    return G0.number_of_nodes()/dists_triu.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter grid search\n",
    "\n",
    "Check giant component fraction against time window and similarity threshold.\n",
    "Setting the grid to search in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60242/189781808.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  params = np.vstack(map(np.ravel, g)).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(217, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_SIM, MAX_SIM, DELTA_SIM = 0.70, 1.0, 0.01\n",
    "sims = np.arange(MIN_SIM, MAX_SIM, DELTA_SIM)\n",
    "MIN_GAM, MAX_GAM, DELTA_GAM = 8*24, 15*24, 24 # in hours\n",
    "gammas = np.arange(MIN_GAM, MAX_GAM, DELTA_GAM)\n",
    "g = np.meshgrid(sims, gammas)\n",
    "params = np.vstack(map(np.ravel, g)).T\n",
    "params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the computations of the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da157642b17a49bfb407525950f566d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?hyperparameters/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_iterator = tqdm(\n",
    "    params,\n",
    "    leave=True,\n",
    "    unit='hyperparameters',\n",
    ")\n",
    "\n",
    "list_of_gcn = Parallel(n_jobs=N_THREADS)(delayed(get_gc_nodes)(i) for i in params_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(g[0], g[1], list_of_gcn)\n",
    "\n",
    "ax.set_title(\"Hyperparameter grid search\")\n",
    "ax.set_xlabel(\"similarity threshold\")\n",
    "ax.set_ylabel(\"time window\")\n",
    "ax.set_zlabel(\"fraction of giant component\")\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time matrix analysis\n",
    "\n",
    "We want to set the time windows such that 95% of articles is accounted for influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_dif(timestamps):\n",
    "    times_dif = np.full(dists_triu.shape, 300)\n",
    "    times_dif = np.triu(times_dif)\n",
    "    timestamps_iterator = tqdm(\n",
    "        timestamps.copy(),\n",
    "        leave=True,\n",
    "        unit='timestamps',\n",
    "    )\n",
    "    for l, i in enumerate(timestamps_iterator):\n",
    "        for c, j in enumerate(timestamps[l:]):\n",
    "            day_dif = (j-i).total_seconds()/3600/24.\n",
    "            times_dif[l,l+c] = day_dif\n",
    "    np.fill_diagonal(times_dif,0)\n",
    "    return times_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difs = create_time_dif(list(info_df['timestamp']))\n",
    "difs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "SIM_THRESHOLD = 0.8\n",
    "\n",
    "days_iterator = tqdm(\n",
    "    range(N_DAYS),\n",
    "    leave=False,\n",
    "    unit='days',\n",
    ")\n",
    "# loop on days of the study\n",
    "for days in days_iterator:\n",
    "    days_column = []\n",
    "    # loop on articles\n",
    "    for i in range(dists_triu.shape[0]-1):\n",
    "        c = i+1\n",
    "        # similarity distances for article c\n",
    "        dists_column = dists_triu[:,c]\n",
    "        # time distances for article c\n",
    "        difs_column = difs[:,c]\n",
    " \n",
    "        # articles that have time difference minor or equal that 'days'\n",
    "        difs_ok = np.where(difs_column <= days)[0]\n",
    "        # if there are those articles\n",
    "        if difs_ok.size != 0:\n",
    "            # get similarity distances of those articles\n",
    "            dists_ok = [dists_column[i] for i in difs_ok]\n",
    "            # get the most similar articles\n",
    "            max_sim = max(dists_ok)\n",
    "            # if this is not similar enough\n",
    "            if max_sim < SIM_THRESHOLD:\n",
    "                # append nan\n",
    "                days_column.append(np.nan)\n",
    "            else:\n",
    "                # append time difference\n",
    "                index = difs_ok[dists_ok.index(max_sim)]\n",
    "                days_column.append(difs_column[index])\n",
    "        else:\n",
    "            days_column.append(np.nan)\n",
    "    df[days] = days_column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(np.arange(1, 31), 30*[18], '--', c='r')\n",
    "df.boxplot(whis=95/50)\n",
    "plt.xlabel('time window (days)',fontsize=18)\n",
    "plt.ylabel('influence range (days)', fontsize=18)\n",
    "plt.grid(False)\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_THRESHOLD = 18 # in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=300)\n",
    "seaborn.violinplot(data=df.dropna())\n",
    "plt.grid(True)\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity analysis\n",
    "\n",
    "We want to set a similarity threshold such that the giant component of the network is populated by more than 80% of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for i in dists_triu:\n",
    "    for j in i:\n",
    "        if j>0:\n",
    "            A.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.hist(A, color='b', bins='auto')\n",
    "plt.title('Pairwise Similarity Distribution')\n",
    "plt.xlabel('similarity')\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for i in dists_triu.T:\n",
    "    pairs.append(max(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.hist(pairs, bins='auto')\n",
    "plt.title('Similarity Distribution for most similar article of each article')\n",
    "plt.xlabel('similarity')\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.arange(0.70, 0.93, 0.015)\n",
    "\n",
    "gammas = [TIME_THRESHOLD]*len(sims)\n",
    "g = [[s, TIME_THRESHOLD] for s in sims]\n",
    "params = np.vstack(map(np.ravel, g)).T\n",
    "\n",
    "sim_thresholds = tqdm(\n",
    "    sims,\n",
    "    leave=False,\n",
    "    unit='similarities',\n",
    ")\n",
    "list_of_gcn_1 = Parallel(n_jobs=N_THREADS)(delayed(get_gc_nodes)(i) for i in sim_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims2 = np.arange(0.5,1,0.01)\n",
    "sim_thresholds = tqdm(\n",
    "    sims2,\n",
    "    leave=False,\n",
    "    unit='thresholds',\n",
    ")\n",
    "list_of_gcn_2 = Parallel(n_jobs=N_THREADS)(delayed(get_gc_nodes)(i) for i in sim_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot(sims2, list_of_gcn_2)\n",
    "plt.xlabel('similarity treshold')\n",
    "plt.ylabel('fraction of articles in the giant component')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15,8))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.plot(sims2, list_of_gcn_2)\n",
    "plt.xlabel('similarity treshold', fontsize=19)\n",
    "plt.ylabel('fraction of articles in the giant component', fontsize=19)\n",
    "ax2 = plt.axes([.2, .2, .25, .3])\n",
    "ax2.plot(sims, list_of_gcn_1)\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_THRESHOLD = 0.75"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd1e9c94448b615a760bc9c3cf750b36b9ab9a397ea85ba08ac48711d7818835"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
