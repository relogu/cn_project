{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical data analysis\n",
    "\n",
    "This notebook will perform the necessary anlysis to find the best parameters for the creation of the empirical network.\n",
    "The parameter to be found are the `minimum similarity`, that will say whether an article has influenced another, and `maximum time distance` between tow similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "seaborn.set_style(\"whitegrid\")\n",
    "\n",
    "## These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# number of days of collection of articles\n",
    "N_DAYS = 30\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'all_stories.json',\n",
    "    'word_matrix.json',\n",
    "    'np_docvs_norm.npz',\n",
    "    'dists_triu.csv',\n",
    "    'info_df.csv',\n",
    "    ]\n",
    "imgs = [\n",
    "    '3Dhyperparameter_grid_search.png',\n",
    "    'time_influence_range.png',\n",
    "    'time_influence_range_violinplot.png',\n",
    "    'similarity_dist_pairwise.png',\n",
    "    'similarity_dist_most_similar.png',\n",
    "    '2Dhyperparameter_grid_search.png',\n",
    "    ]\n",
    "stories = [\n",
    "    'world_russia',\n",
    "    'world_norway',\n",
    "    'world_capitol_hill',\n",
    "]\n",
    "story_to_elaborate = 1\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[4]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    info_df = pd.read_csv(csv_file).drop(['Unnamed: 0'], 1)\n",
    "info_df = info_df.rename(columns={'id': 'article_id'})\n",
    "info_df['timestamp'] = pd.to_datetime(info_df.timestamp)\n",
    "print(info_df.head())\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[3]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    dists_triu = pd.read_csv(csv_file, sep=',', header=None)\n",
    "dists_triu = dists_triu.values\n",
    "print(dists_triu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ids = list(info_df['article_id'])\n",
    "len(list_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for building the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(\n",
    "    data, # DataFrame containing articles info\n",
    "    timestamp, # time of current article\n",
    "    similarities, # similarities of the current article\n",
    "    max_dt, # max dt (in hours) for two articles to be linked\n",
    "    min_dt, # min dt (in hours) for two articles to be linked\n",
    "    min_similarity, # minimum cos sim distance for two articles to be linked\n",
    "    outs, # current list of excluded articles\n",
    "    ):\n",
    "    while True:\n",
    "        # get the maximum similarity w.r.t. older articles\n",
    "        similarity = max(similarities)\n",
    "        # get the index of such max similar article\n",
    "        index = similarities.index(similarity)\n",
    "        # get dt in terms of hours\n",
    "        dt = (timestamp - data['timestamp'][index]).total_seconds() / 3600# / 24\n",
    "        # similarity threshold\n",
    "        if similarity < min_similarity:\n",
    "            # return None to add index to outs\n",
    "            return None\n",
    "        # continue if article is in outs or its distant in time\n",
    "        elif index in outs or dt > max_dt or dt <= min_dt:\n",
    "            similarities[index] = 0\n",
    "        # pass condition\n",
    "        else:\n",
    "            return index\n",
    "\n",
    "def create_graph(\n",
    "    dists_triu, # similarity matrix\n",
    "    data, # DataFrame containing articles info\n",
    "    time_max: int = 168, # max dt (in hours) for two articles to be linked\n",
    "    time_min: int = 4, # min dt (in hours) for two articles to be linked\n",
    "    sim_min: float = 0.8, # minimum cos sim distance for two articles to be linked\n",
    "    ):\n",
    "    # max number of nodes\n",
    "    n_articles = dists_triu.shape[0]\n",
    "    # instantiate the graph (NOT DIRECTED HERE!)\n",
    "    G = nx.Graph()\n",
    "    # adding the first node\n",
    "    G.add_node(0, step=0,\n",
    "               date=data['timestamp'][0],\n",
    "               domain=data['source'][0],\n",
    "               _id=data['article_id'][0],\n",
    "               children=[])\n",
    "    # instatiating elimination list\n",
    "    outs = []\n",
    "    # loop on the other articles\n",
    "    for i in range(1, n_articles):\n",
    "        # get time of current article\n",
    "        pub_i = data['timestamp'][i]\n",
    "        # get similarities\n",
    "        column = list(dists_triu[:, i])\n",
    "        # get index of an article related to the current one\n",
    "        index = get_index(data, pub_i, column, time_max, time_min, sim_min, outs)\n",
    "        # if a relation was found\n",
    "        if index != None:\n",
    "            # if the related article has not already been inserted, insert it\n",
    "            if index not in G.nodes():\n",
    "                G.add_nodes_from([(index, {\n",
    "                    'timestamp': data['timestamp'][index],\n",
    "                    'source': data['source'][index],\n",
    "                    'id': data['article_id'][index],\n",
    "                })])\n",
    "            # if the current article has not already been inserted, insert it\n",
    "            if i not in G.nodes():\n",
    "                G.add_nodes_from([(i, {\n",
    "                    'timestamp': data['timestamp'][i],\n",
    "                    'source': data['source'][i],\n",
    "                    'id': data['article_id'][i]\n",
    "                })])\n",
    "            # linking the nodes\n",
    "            G.add_edge(index, i)\n",
    "        # if a relation was not found\n",
    "        else:\n",
    "            # add current article to elimination listS\n",
    "            outs.append(i)\n",
    "    # return the graph\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting the giant componend of a created graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_nodes(\n",
    "    params\n",
    "):\n",
    "    G = create_graph(dists_triu, info_df, time_max=params[1], sim_min=params[0])\n",
    "    Gcc = sorted((G.subgraph(c) for c in nx.connected_components(G)), key = len, reverse=True)\n",
    "    G0 = Gcc[0]       \n",
    "    return G0.number_of_nodes()/dists_triu.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter grid search\n",
    "\n",
    "Check giant component fraction against time window and similarity threshold.\n",
    "Setting the grid to search in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SIM, MAX_SIM, DELTA_SIM = 0.70, 1.0, 0.01\n",
    "sims = np.arange(MIN_SIM, MAX_SIM, DELTA_SIM)\n",
    "MIN_GAM, MAX_GAM, DELTA_GAM = 7*24, 12*24, 12 # in hours\n",
    "gammas = np.arange(MIN_GAM, MAX_GAM, DELTA_GAM)\n",
    "g = np.meshgrid(sims, gammas)\n",
    "params = np.vstack(map(np.ravel, g)).T\n",
    "params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the computations of the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_iterator = tqdm(\n",
    "    params,\n",
    "    leave=True,\n",
    "    unit='hyperparameters',\n",
    ")\n",
    "\n",
    "list_of_gcn = Parallel(n_jobs=N_THREADS)(delayed(get_gc_nodes)(i) for i in params_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#ax.scatter(g[0], g[1], list_of_gcn)\n",
    "# Plot the surface.\n",
    "Z =  np.array(list_of_gcn).reshape(g[0].shape)\n",
    "np.savez(str(PATH_TO_DATA/stories[story_to_elaborate]/'hyperparams.npz'), g)\n",
    "np.savez(str(PATH_TO_DATA/stories[story_to_elaborate]/'frac_gc.npz'), Z)\n",
    "# g = np.load(str(PATH_TO_DATA/stories[story_to_elaborate]/'hyperparams.npz'))['arr_0']\n",
    "# Z = np.load(str(PATH_TO_DATA/stories[story_to_elaborate]/'frac_gc.npz'))['arr_0']\n",
    "surf = ax.plot_surface(g[0], g[1]/24, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ax.set_title(\"Hyperparameter grid search\")\n",
    "ax.set_xlabel(\"similarity threshold\")\n",
    "ax.set_ylabel(\"time window (hours)\")\n",
    "ax.set_zlabel(\"fraction of giant component\")\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time matrix analysis\n",
    "\n",
    "We want to set the time windows such that 95% of articles is accounted for influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_dif(timestamps):\n",
    "    times_dif = np.full(dists_triu.shape, 300.0)\n",
    "    timestamps_iterator = tqdm(\n",
    "        timestamps.copy(),\n",
    "        leave=True,\n",
    "        unit='timestamps',\n",
    "    )\n",
    "    for l, i in enumerate(timestamps_iterator):\n",
    "        for k, j in enumerate(timestamps[l:]):\n",
    "            day_dif = abs((i-j).total_seconds()/3600)/24.\n",
    "            times_dif[l,l+k] = day_dif\n",
    "    np.fill_diagonal(times_dif,0.0)\n",
    "    return times_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difs = create_time_dif(list(info_df['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "occ = plt.hist(difs[0], bins='auto')\n",
    "plt.title('Distribution of time distances w.r.t. the first published articles')\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('number of articles')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(np.diag(difs, k=1), bins=len(occ[0]))\n",
    "plt.title('Distribution of time distances w.r.t. the previous published articles')\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('number of articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "SIM_THRESHOLD = 0.8\n",
    "\n",
    "n_articles = dists_triu.shape[0]\n",
    "\n",
    "min_dif_temp = 4/24 #  in days\n",
    "initial_infected = len(np.where(difs[0]<=min_dif_temp)[0])\n",
    "\n",
    "days_iterator = tqdm(\n",
    "    range(1, N_DAYS+1),\n",
    "    leave=True,\n",
    "    unit='days',\n",
    ")\n",
    "# TODO: check this\n",
    "# loop on days of the study\n",
    "for days in days_iterator:\n",
    "    days_column = []\n",
    "    # loop on articles\n",
    "    # for i in range(dists_triu.shape[0]-1):\n",
    "    #     c = i+1\n",
    "    for i in range(initial_infected, n_articles-initial_infected):\n",
    "        c=i\n",
    "        # similarity distances for article c\n",
    "        dists_column = dists_triu[:,c]\n",
    "        # time distances for article c\n",
    "        difs_column = difs[:,c]\n",
    " \n",
    "        # articles that have time difference minor or equal that 'days'\n",
    "        difs_ok = np.where(((difs_column <= days) & (difs_column > min_dif_temp)))[0]\n",
    "        # if there are those articles\n",
    "        if difs_ok.size != 0:\n",
    "            # get similarity distances of those articles\n",
    "            dists_ok = [dists_column[i] for i in difs_ok]\n",
    "            # get the most similar articles\n",
    "            max_sim = max(dists_ok)\n",
    "            # if this is not similar enough\n",
    "            if max_sim < SIM_THRESHOLD:\n",
    "                # append nan\n",
    "                days_column.append(np.nan)\n",
    "            else:\n",
    "                # append time difference\n",
    "                index = difs_ok[dists_ok.index(max_sim)]\n",
    "                days_column.append(difs_column[index])\n",
    "        else:\n",
    "            days_column.append(np.nan)\n",
    "    df[days] = days_column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(np.arange(1, 31), 30*[16], '--', c='r')\n",
    "df.boxplot(whis=95/50)\n",
    "plt.xlabel('time window (days)',fontsize=18)\n",
    "plt.ylabel('influence range (days)', fontsize=18)\n",
    "plt.grid(False)\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_THRESHOLD = 9 # in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=300)\n",
    "seaborn.violinplot(data=df.dropna())\n",
    "plt.grid(True)\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity analysis\n",
    "\n",
    "We want to set a similarity threshold such that the giant component of the network is populated by more than 80% of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = dists_triu.flatten()\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.hist(A[A>0], bins=50)\n",
    "plt.title('Pairwise Similarity Distribution')\n",
    "plt.xlabel('similarity')\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.hist(np.max(dists_triu, axis=1), bins=50)\n",
    "plt.title('Similarity Distribution for most similar article of each article')\n",
    "plt.xlabel('similarity')\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SIM, MAX_SIM, DELTA_SIM = 0.70, 1.0, 0.01\n",
    "sims_1 = np.arange(MIN_SIM, MAX_SIM, DELTA_SIM)\n",
    "TIME_THRESHOLD = 9 # in days\n",
    "params = [[s, TIME_THRESHOLD] for s in sims_1]\n",
    "sim_thresholds = tqdm(\n",
    "    params,\n",
    "    leave=True,\n",
    "    unit='similarities',\n",
    ")\n",
    "list_of_gcn_1 = Parallel(n_jobs=N_THREADS)(delayed(get_gc_nodes)(i) for i in sim_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot(sims_1, list_of_gcn_1)\n",
    "plt.xlabel('similarity treshold')\n",
    "plt.ylabel('fraction of articles in the giant component')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SIM, MAX_SIM, DELTA_SIM = 0.50, 1.0, 0.01\n",
    "sims_2 = np.arange(MIN_SIM, MAX_SIM, DELTA_SIM)\n",
    "TIME_THRESHOLD = 14 # in days\n",
    "params = [[s, TIME_THRESHOLD] for s in sims_2]\n",
    "sim_thresholds = tqdm(\n",
    "    params,\n",
    "    leave=True,\n",
    "    unit='thresholds',\n",
    ")\n",
    "list_of_gcn_2 = Parallel(n_jobs=N_THREADS)(delayed(get_gc_nodes)(i) for i in sim_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15,8))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.plot(sims_2, list_of_gcn_2)\n",
    "plt.xlabel('similarity treshold', fontsize=19)\n",
    "plt.ylabel('fraction of articles in the giant component', fontsize=19)\n",
    "ax2 = plt.axes([.2, .2, .25, .3])\n",
    "ax2.plot(sims_1, list_of_gcn_1)\n",
    "plt.show()\n",
    "plt.savefig(PATH_TO_DATA/stories[story_to_elaborate]/imgs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_THRESHOLD = 0.75"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd1e9c94448b615a760bc9c3cf750b36b9ab9a397ea85ba08ac48711d7818835"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
