{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search the articles\n",
    "\n",
    "This notebook will search the articles for the project exploiting the [Media Cloud](https://mediacloud.org/) database.\n",
    "One have to set the customizable parameters:\n",
    "\n",
    "1. `PATH_TO_DATA` is the Path object for saving the constructed DataFrames;\n",
    "2. `MY_KEY` parameter is the user key every Media Cloud user has been provided once signed up, for more info go [here](https://github.com/mediacloud/backend/blob/master/doc/api_2_0_spec/api_2_0_spec.md#authentication);\n",
    "3. `MIN_LEGTH` parameter is the minimiun length that a word must have to be considered for the word vector;\n",
    "4. `MIN_FREQUENCY` parameter is the minimum frequency a word must appear in at least one article to be considered for the word vector;\n",
    "5. `MAX_ARTICLES` parameter set the maximum number of articles to search;\n",
    "6. `N_THREADS` parameter set the number of threads for parallelizing some of the procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import mediacloud.api\n",
    "from IPython.display import JSON\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "## These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# Media Cloud account keys\n",
    "LOLLO_KEY_1 = 'fa108cf51bdb186f9f037bc196d0183b18b24caac3158416a858b5a9b58dc143'\n",
    "LOLLO_KEY_2 = '66aa9cf8dbd642b0e47f6811764cbe451a84d9429b8d2b3647c97c0af8fd40f5'\n",
    "DANI_KEY = '00692be452b478dc158269f890533127ceb444b9f0cc05411ad154f67d55fec1'\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediatags for restricting queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITALY_M_TAG = 'tags_id_media:38380117'\n",
    "US_M_TAG = 'tags_id_media:38379429'\n",
    "UK_M_TAG = 'tags_id_media:38381111'\n",
    "FRANCE_M_TAG = 'tags_id_media:38379799'\n",
    "GERMANY_M_TAG = 'tags_id_media:38379816'\n",
    "SPAIN_M_TAG = 'tags_id_media:38002034'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters for language.\n",
    "Recall that the Word2Vect model used in the following steps have been trained only from english words!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_LANG = 'language:it'\n",
    "EN_LANG = 'language:en'\n",
    "FR_LANG = 'language:fr'\n",
    "DE_LANG = 'language:de'\n",
    "SP_LANG = 'language:sp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting info about articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: limit max number of stories\n",
    "# TODO: check for repeated stories\n",
    "def all_matching_stories(mc_client, q, fq):\n",
    "    \"\"\"\n",
    "    Return all the stories matching a query within Media Cloud. Page through the results automatically.\n",
    "    :param mc_client: a `mediacloud.api.MediaCloud` object instantiated with your API key already\n",
    "    :param q: your boolean query\n",
    "    :param fq: your date range query\n",
    "    :return: a list of media cloud story items\n",
    "    \"\"\"\n",
    "    last_id = 0\n",
    "    more_stories = True\n",
    "    stories = []\n",
    "    while more_stories:\n",
    "        page = mc_client.storyList(q, fq, last_processed_stories_id=last_id, rows=500, sort='processed_stories_id')\n",
    "        print(\"  got one page with {} stories\".format(len(page)))\n",
    "        if len(page) == 0:\n",
    "            more_stories = False\n",
    "        else:\n",
    "            stories += page[:-1]\n",
    "            last_id = page[-1]['processed_stories_id']\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for processing info from stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_info(story):\n",
    "    return pd.DataFrame(\n",
    "            [{\n",
    "                'article_id': story['stories_id'],\n",
    "                'timestamp': str(story['publish_date']),\n",
    "                'source': story['media_id'],\n",
    "            }]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the output folders if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'all_stories.json',\n",
    "    'word_matrix.json',\n",
    "    ]\n",
    "stories = [\n",
    "    'world_russia',\n",
    "    'world_norway',\n",
    "    'world_capitol_hill',\n",
    "]\n",
    "os.makedirs(PATH_TO_DATA, exist_ok=True)\n",
    "[os.makedirs(PATH_TO_DATA/story, exist_ok=True) for story in stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the Media Cloud client, and getting some info about the status of the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = mediacloud.api.MediaCloud(DANI_KEY)\n",
    "print('Media cloud version '+str(mediacloud.__version__))\n",
    "# make sure your connection and API key work by asking for the high-level system statistics\n",
    "# and print it out as a nice json tree - we'll use this later (only works in Jupyter Lab)\n",
    "JSON(mc.stats())\n",
    "# italy collection 38380117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query definition.\n",
    "The following cells will run the queries that will get articles info and the word matrix (used in the project).\n",
    "Some stats will be visualized.\n",
    "One has to customize the filters for getting what he/she is interested in.\n",
    "Remind that the epidemic model works fine for events that have a rapid spreading (popular, interesting) and then die, because of that one has to pay attention not only to the argument but also to the time windows.\n",
    "For more info about constructing the query look [here](https://github.com/mediacloud/backend/blob/master/doc/api_2_0_spec/api_2_0_spec.md#query-parameters-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russia shooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_russia_query = '(russia AND school AND shooting) OR (russia AND scuola AND sparatoria) OR (russie AND école AND fusillade) OR (rusia AND colegio AND tiroteo) OR  (russland AND schule AND (angriff OR schießen))'\n",
    "world_russia_query = '(russia AND school AND shooting) AND '+EN_LANG\n",
    "start_date = datetime.date(2021, 5, 10)\n",
    "end_date = datetime.date(2021, 6, 10)\n",
    "if not Path(PATH_TO_DATA/stories[0]/files[0]).exists():\n",
    "    date_range = mc.dates_as_query_clause(start_date, end_date)\n",
    "    story_count = mc.storyCount(world_russia_query, date_range)['count']\n",
    "    print('Media Cloud found {} stories'.format(story_count)) # 4322\n",
    "    all_stories = all_matching_stories(\n",
    "        mc,\n",
    "        world_russia_query,\n",
    "        date_range)\n",
    "    with open(PATH_TO_DATA/stories[0]/files[0], 'x') as json_file:\n",
    "        json.dump(all_stories, json_file)\n",
    "else:\n",
    "    with open(PATH_TO_DATA/stories[0]/files[0], 'r') as json_file:\n",
    "        all_stories = json.load(json_file)\n",
    "print('Processing {} stories'.format(len(list(all_stories))))\n",
    "stories_iterator = tqdm(\n",
    "    list(all_stories),\n",
    "    leave=True,\n",
    "    unit='stories',\n",
    ")\n",
    "articles_info = Parallel(n_jobs=N_THREADS)(delayed(process_info)(i) for i in stories_iterator)\n",
    "info_df = pd.concat(articles_info, axis=0).reset_index().drop('index', 1)\n",
    "info_df['timestamp'] = pd.to_datetime(info_df.timestamp)\n",
    "info_df = info_df.sort_values(by='timestamp')\n",
    "del articles_info\n",
    "print(info_df.head())\n",
    "info_df['time_diff'] = info_df['timestamp'].map(lambda x: round((x-info_df['timestamp'][0]).total_seconds()/3600/24))\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot(list(Counter(info_df['time_diff']).values()))\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('articles published')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "if not Path(PATH_TO_DATA/stories[0]/files[1]).exists():\n",
    "    print('Getting word matrix')\n",
    "    stories_words = mc.storyWordMatrix(\n",
    "        world_russia_query,\n",
    "        date_range,\n",
    "        rows=len(list(all_stories)))\n",
    "    with open(PATH_TO_DATA/stories[0]/files[1], 'w') as json_file:\n",
    "        json.dump(stories_words, json_file)\n",
    "else:\n",
    "    with open(PATH_TO_DATA/stories[0]/files[1], 'r') as json_file:\n",
    "        all_stories = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norway terroristic attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_norway_query = '(attack AND norway) OR (attacco AND norvegia) OR (ataque AND noruega) OR (attacke AND norwegen) OR (attaque AND norvège)'\n",
    "world_norway_query = '(attack AND norway) AND '+EN_LANG\n",
    "start_date = datetime.date(2021, 7, 21)\n",
    "end_date = datetime.date(2021, 8, 21)\n",
    "if not Path(PATH_TO_DATA/stories[1]/files[0]).exists():\n",
    "    date_range = mc.dates_as_query_clause(start_date, end_date)\n",
    "    story_count = mc.storyCount(world_norway_query, date_range)['count']\n",
    "    print('Media Cloud found {} stories'.format(story_count)) # 3875\n",
    "    all_stories = all_matching_stories(\n",
    "        mc,\n",
    "        world_norway_query,\n",
    "        date_range)\n",
    "    with open(PATH_TO_DATA/stories[1]/files[0], 'x') as json_file:\n",
    "        json.dump(all_stories, json_file)\n",
    "else:\n",
    "    with open(PATH_TO_DATA/stories[1]/files[0], 'r') as json_file:\n",
    "        all_stories = json.load(json_file)\n",
    "print('Processing {} stories'.format(len(list(all_stories))))\n",
    "stories_iterator = tqdm(\n",
    "    list(all_stories),\n",
    "    leave=True,\n",
    "    unit='stories',\n",
    ")\n",
    "articles_info = Parallel(n_jobs=N_THREADS)(delayed(process_info)(i) for i in stories_iterator)\n",
    "info_df = pd.concat(articles_info, axis=0).reset_index().drop('index', 1)\n",
    "info_df['timestamp'] = pd.to_datetime(info_df.timestamp)\n",
    "info_df = info_df.sort_values(by='timestamp')\n",
    "del articles_info\n",
    "print(info_df.head())\n",
    "info_df['time_diff'] = info_df['timestamp'].map(lambda x: round((x-info_df['timestamp'][0]).total_seconds()/3600/24))\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot(list(Counter(info_df['time_diff']).values()))\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('articles published')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "if not Path(PATH_TO_DATA/stories[1]/files[1]).exists():\n",
    "    print('Getting word matrix')\n",
    "    stories_words = mc.storyWordMatrix(\n",
    "        world_norway_query,\n",
    "        date_range,\n",
    "        rows=len(list(all_stories)))\n",
    "    with open(PATH_TO_DATA/stories[1]/files[1], 'w') as json_file:\n",
    "        json.dump(stories_words, json_file)\n",
    "else:\n",
    "    with open(PATH_TO_DATA/stories[1]/files[1], 'r') as json_file:\n",
    "        all_stories = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capitol hill mob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_capitol_hill_query = 'capitol AND hill AND (assault OR mob OR asalto OR agression OR angriff OR assalto)'\n",
    "world_capitol_hill_query = 'capitol AND hill AND (assault OR mob) AND '+EN_LANG\n",
    "start_date = datetime.date(2021, 1, 5)\n",
    "end_date = datetime.date(2021, 2, 5)\n",
    "if not Path(PATH_TO_DATA/stories[2]/files[0]).exists():\n",
    "    date_range = mc.dates_as_query_clause(start_date, end_date)\n",
    "    story_count = mc.storyCount(world_capitol_hill_query, date_range)['count']\n",
    "    print('Media Cloud found {} stories'.format(story_count)) # 45004\n",
    "    all_stories = all_matching_stories(\n",
    "        mc,\n",
    "        world_capitol_hill_query,\n",
    "        date_range)\n",
    "    with open(PATH_TO_DATA/stories[2]/files[0], 'x') as json_file:\n",
    "        json.dump(all_stories, json_file)\n",
    "else:\n",
    "    with open(PATH_TO_DATA/stories[2]/files[0], 'r') as json_file:\n",
    "        all_stories = json.load(json_file)\n",
    "print('Processing {} stories'.format(len(list(all_stories))))\n",
    "stories_iterator = tqdm(\n",
    "    list(all_stories),\n",
    "    leave=True,\n",
    "    unit='stories',\n",
    ")\n",
    "articles_info = Parallel(n_jobs=N_THREADS)(delayed(process_info)(i) for i in stories_iterator)\n",
    "info_df = pd.concat(articles_info, axis=0).reset_index().drop('index', 1)\n",
    "info_df['timestamp'] = pd.to_datetime(info_df.timestamp)\n",
    "info_df = info_df.sort_values(by='timestamp')\n",
    "del articles_info\n",
    "print(info_df.head())\n",
    "info_df['time_diff'] = info_df['timestamp'].map(lambda x: round((x-info_df['timestamp'][0]).total_seconds()/3600/24))\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.plot(list(Counter(info_df['time_diff']).values()))\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('articles published')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "if not Path(PATH_TO_DATA/stories[2]/files[1]).exists():\n",
    "    print('Getting word matrix')\n",
    "    stories_words = mc.storyWordMatrix(\n",
    "        world_capitol_hill_query,\n",
    "        date_range,\n",
    "        rows=len(list(all_stories)))\n",
    "    with open(PATH_TO_DATA/stories[2]/files[1], 'w') as json_file:\n",
    "        json.dump(stories_words, json_file)\n",
    "else:\n",
    "    with open(PATH_TO_DATA/stories[2]/files[1], 'r') as json_file:\n",
    "        all_stories = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charlie Hebdo terrorist attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: proper filtering to reduce the number of stories\n",
    "query = 'charlie hebdo AND '+UK_M_TAG\n",
    "start_date = datetime.date(2015, 1, 7)\n",
    "end_date = datetime.date(2015, 2, 7)\n",
    "date_range = mc.dates_as_query_clause(start_date, end_date)\n",
    "story_count = mc.storyCount(query,\n",
    "                            date_range)['count']\n",
    "print('Media Cloud found {} stories'.format(story_count)) # WORLD 141156, ITALY 999, FRANCE 4310, US 3613, UK 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_to_elaborate = 0\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[0]) as json_file:\n",
    "    all_stories = json.load(json_file)\n",
    "# JSON(all_stories)\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[1]) as json_file:\n",
    "    stories_words = json.load(json_file)\n",
    "# JSON(stories_words)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd1e9c94448b615a760bc9c3cf750b36b9ab9a397ea85ba08ac48711d7818835"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
