{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec5ae0",
   "metadata": {},
   "source": [
    "# Computing Graphs and initial conditions\n",
    "\n",
    "This notebook will instatiate and save the necessities for conducting the dynamical analysis.\n",
    "Firstly the dataset will be elaborated and then graphs and initial conditions will be computed.\n",
    "Might be necessary to change the data path (`PATH_TO_DATA`) or the dataset filnames (`WORD_VECTORS_FILENAME` and `ARTCLES_DF_FILENAME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747b823-cf56-4939-b9d5-8c86e64c94ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:30:02.566159Z",
     "start_time": "2022-01-08T16:30:02.205349Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from gensim import corpora, models\n",
    "from gensim.models.word2vec import Word2Vec, KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "# import cugraph ## this library works with CUDA-capable GPUs but has many issues\n",
    "\n",
    "PATH_TO_DATA = Path('../data')\n",
    "WORD_VECTORS_FILENAME = 'words_dataframe.csv'\n",
    "ARTICLES_DF_FILENAME = 'info_dataframe.csv'\n",
    "N_THREADS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49565de-1bd6-44ef-aae8-55ad2dedd36c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Similarity Matrix\n",
    "\n",
    "In order to create the **Similarity Matrix**, we should _load_ the 'WORD VECTORS' DataFrame and then we should _calculate the distances_ of all the articles (exploiting the simple dot product).\n",
    "'WORD_VECTORS' must be passed through the `create_model_matrix` function that will map each document to its 300-dim vector representation using a model from `gensim`.\n",
    "This model can be dowloaded using the downloading API of `gensim`, more information can be found [here](https://github.com/RaRe-Technologies/gensim-data).\n",
    "The available models to choose are [ConceptNet Numberbatch](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972) or [Google News Word2Vec](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "The results of `create_model_matrix` must be normalized before computing the distance.\n",
    "The fastest method for normalization is that from _sklearn_, but also a _numpy_ version is provided (but not used).\n",
    "\n",
    "This is equivalent to use the _cosine similarity_ distance to compute these distances.\n",
    "\n",
    "Loading the DataFrame is the first thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "word_vectors = pd.read_csv(PATH_TO_DATA/WORD_VECTORS_FILENAME).drop(['Unnamed: 0','article_id'], 1)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e1599",
   "metadata": {},
   "source": [
    "Then, the function for creating the whole word matrix is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_matrix(data, model):\n",
    "    # get the list of words from data\n",
    "    texts = [list(map(lambda x: x.encode().decode(\"utf8\"), list(data.columns)))]\n",
    "\n",
    "    # corpora with all words from data, dictionary[id] returns the word linked to id\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # instatiate the iterator\n",
    "    articles_iterator = tqdm(\n",
    "        range(len(data)),\n",
    "        leave=True,\n",
    "        unit='articles',\n",
    "    )\n",
    "\n",
    "    # function to parallelize \n",
    "    def fn(article):\n",
    "        for word in list(data.columns):\n",
    "            weight = data[word][article] / data[word].sum()\n",
    "            w = dictionary.doc2idx([word])\n",
    "            try:\n",
    "                word_vector = model.word_vec(w[0])\n",
    "                try:\n",
    "                    assert np.isfinite(word_vector).all()\n",
    "                except AssertionError:\n",
    "                    print(w)\n",
    "            except KeyError:\n",
    "                word_vector = np.zeros((1, 300))\n",
    "        return word_vector*weight\n",
    "            \n",
    "    list_of_docvs = Parallel(n_jobs=N_THREADS)(delayed(fn)(i) for i in articles_iterator)\n",
    "    return list_of_docvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51499d2f",
   "metadata": {},
   "source": [
    "Loading the [ConceptNet Numberbatch](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptnet = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'conceptnet-numberbatch-17-06-300.gz')\n",
    "conceptnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f731e9",
   "metadata": {},
   "source": [
    "Loading the [Google News Word2Vec](https://code.google.com/archive/p/word2vec/) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e17e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news_word2vec = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'word2vec-google-news-300.gz', binary=True)\n",
    "google_news_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def882bb",
   "metadata": {},
   "source": [
    "Get the whole word matrix using the DataFrame and the model choosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = google_news_word2vec\n",
    "list_of_docvs = create_model_matrix(word_vectors, model)\n",
    "list_of_docvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144cebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74bf6188",
   "metadata": {},
   "source": [
    "`numpy` normalization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8253e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative normalization procedure\n",
    "row_sums = docvs.sum(axis=1)\n",
    "np_docvs_norm = docvs / np.sqrt((row_sums**2).sum(-1))[:, np.newaxis]\n",
    "print(\"Shape of normalized matrix is {}.\".format(np_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(np_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(np_docvs_norm), np.min(np_docvs_norm)))\n",
    "np_docvs_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000911c",
   "metadata": {},
   "source": [
    "`sklearn` normalization procedure (axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b28e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn normalization procedure (axis=1)\n",
    "sk_docvs_norm = normalize(docvs)\n",
    "print(\"Shape of normalized matrix is {}.\".format(sk_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(sk_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(sk_docvs_norm), np.min(sk_docvs_norm)))\n",
    "sk_docvs_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded9e46",
   "metadata": {},
   "source": [
    "`sklearn` normalization procedure (axis=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn normalization procedure (axis=0)\n",
    "sk_docvs_norm_0 = normalize(docvs)\n",
    "print(\"Shape of normalized matrix is {}.\".format(sk_docvs_norm._0shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(sk_docvs_norm)_0))\n",
    "print(\"Max={}; Min={}.\".format(np.max(sk_docvs_norm)_0, np.min(sk_docvs_norm)_0))\n",
    "sk_docvs_norm_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712f297",
   "metadata": {},
   "source": [
    "Computation of the distance matrix.\n",
    "The simple dot product is used between the matrix and its transpose.\n",
    "Here are used `scipy.sparse` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee5180-2a9b-40bb-9105-843e378effe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sparse.csr_matrix(sk_docvs_norm)\n",
    "s_t = sparse.csr_matrix(sk_docvs_norm).T\n",
    "s_dist = s.dot(s_t)\n",
    "dists_triu = sparse.triu(s_dist, k=1)\n",
    "dists_triu = np.array(dists_triu.todense())\n",
    "np.savetxt('../data/dists_triu.csv', dists_triu, delimiter=',')\n",
    "dists_triu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40624608",
   "metadata": {},
   "source": [
    "These methods, used for building the graph, have been extracted and adapted from [this repo](https://github.com/elisamussumeci/modeling-news-spread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79ef81-343c-4ff8-abf8-44c3ebc17b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:32:36.476447Z",
     "start_time": "2022-01-08T16:32:36.427851Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pos(data, pub_i, column_list, time_max, sim_min, outs):\n",
    "    ans = False\n",
    "    pos = None\n",
    "\n",
    "    while ans is False:\n",
    "        sim = max(column_list)\n",
    "        pos = column_list.index(sim)\n",
    "        time_dif = (pub_i - data['timestamp'][pos]).total_seconds() / 3600\n",
    "        if sim < sim_min:\n",
    "            pos = None\n",
    "            ans = True\n",
    "        elif pos in outs or time_dif > time_max:\n",
    "            column_list[pos] = 0\n",
    "        else:\n",
    "            ans = True\n",
    "    return pos\n",
    "\n",
    "def create_graph(dists_triu, data, time_max=168, sim_min=0.8):\n",
    "    size = dists_triu.shape[0]\n",
    "    G = nx.DiGraph()\n",
    "    G.add_node(0, step=0, date=data['timestamp'][0], domain=data['source'][0], _id=data['article_id'][0],\n",
    "              children=[])\n",
    "    outs = []\n",
    "    for i in range(1,size):\n",
    "        pub_i = data['timestamp'][i]\n",
    "        column = list(dists_triu[:, i])\n",
    "        pos = get_pos(data, pub_i, column, time_max, sim_min, outs)\n",
    "\n",
    "        if pos != None:\n",
    "            if pos not in G.nodes():\n",
    "                domain_1 = data['source'][pos]\n",
    "                G.add_node(pos, date=data['timestamp'][pos], domain=domain_1,\n",
    "                           _id=data['article_id'][pos], children=[])\n",
    "            if i not in G.nodes():\n",
    "                domain_2 = data['source'][i]\n",
    "                G.add_node(i, date=pub_i, domain=domain_2, _id=data['article_id'][i], children=[])\n",
    "\n",
    "            G.add_edge(pos, i)\n",
    "        else:\n",
    "            outs.append(i)\n",
    "    return G\n",
    "\n",
    "def create_date(pub1, pub2, s):\n",
    "    dif = (pub2-pub1).total_seconds()/3600\n",
    "    return round((dif/s))\n",
    "\n",
    "def create_graphml(dists_triu, data, time_max=168, sim_min=0.8):\n",
    "    size = dists_triu.shape[0]\n",
    "    G = nx.DiGraph()\n",
    "    G.add_node(0, step=0, date=0, domain=data['source'][0])\n",
    "    date_init = data['timestamp'][0]\n",
    "    outs = []\n",
    "    for i in range(1, size):\n",
    "        pub_i = data['timestamp'][i]\n",
    "        column = list(dists_triu[:,i])\n",
    "        pos = get_pos(data, pub_i, column, time_max, sim_min, outs)\n",
    "\n",
    "        if pos != None:\n",
    "            if pos not in G.nodes():\n",
    "                domain_1 = data['source'][pos]\n",
    "                date_1 = create_date(date_init, data['timestamp'], 5)\n",
    "                G.add_node(pos, date=date_1, domain=domain_1)\n",
    "            if i not in G.nodes():\n",
    "                domain_2 = data['source'][i]\n",
    "                date_2 = create_date(date_init, pub_i, 5)\n",
    "                G.add_node(i, date=date_2, domain=domain_2)\n",
    "\n",
    "            G.add_edge(pos, i)\n",
    "        else:\n",
    "            outs.append(i)\n",
    "    return G\n",
    "\n",
    "def create_matrix_domain(graph):\n",
    "    \n",
    "    domain_list = []\n",
    "    for pos in graph.nodes():\n",
    "        node = graph.nodes()[pos]\n",
    "        d = node['domain']\n",
    "        if d not in domain_list:\n",
    "            domain_list.append(d)\n",
    "\n",
    "    df = pd.DataFrame(0, index = domain_list, columns = domain_list)\n",
    "\n",
    "    for pos in graph.nodes():\n",
    "        node = graph.nodes()[pos]\n",
    "        d = node['domain']\n",
    "        successors = graph.successors(pos)\n",
    "        for suc in successors:\n",
    "            df[d][graph.nodes()[suc]['domain']] += 1\n",
    "\n",
    "    return [domain_list, df]\n",
    "\n",
    "def create_complete_adjacency(graph, matrix):\n",
    "    df = pd.DataFrame(0, index=graph.nodes(), columns=graph.nodes())\n",
    "    for column in graph.nodes():\n",
    "        i_domains_column = matrix[graph.nodes()[column]['domain']]\n",
    "        for row in graph.nodes():\n",
    "            prob = i_domains_column[graph.nodes()[row]['domain']]\n",
    "            df[column][row] = prob\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2772fc",
   "metadata": {},
   "source": [
    "Creating, saving and drawing the Graphs instatiated using _networkX_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06e015-eb6f-4088-9e70-b83ace836917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:32:39.201018Z",
     "start_time": "2022-01-08T16:32:39.189349Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv(PATH_TO_DATA/ARTICLES_DF_FILENAME)\n",
    "articles['timestamp'] = pd.to_datetime(articles.timestamp)\n",
    "articles = articles.drop('Unnamed: 0', 1)\n",
    "articles = articles.rename(columns={'id': 'article_id'})\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f97f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph(dists_triu, articles)\n",
    "nx.write_gpickle(G, '../data/empirical_graph.gpickle')\n",
    "# nx.draw(G, with_labels=True, font_weight='bold')\n",
    "# plt.show()\n",
    "H = create_graphml(dists_triu, articles)\n",
    "nx.write_graphml(H, '../data/empirical_graph.graphml')\n",
    "# nx.draw(H, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(G.nodes())).transpose().to_csv('../data/empirical_graph_nodes.csv')\n",
    "all_nodes_domains = []\n",
    "for i in G.nodes():\n",
    "    all_nodes_domains.append(G.nodes()[i]['domain'])\n",
    "\n",
    "f = open('../data/graph_original_domains_each_node.txt', 'w')\n",
    "for item in all_nodes_domains:\n",
    "    f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa14860",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_list, domain_matrix = create_matrix_domain(G)\n",
    "graph_complete = create_complete_adjacency(G, domain_matrix)\n",
    "as_numpy = np.array(graph_complete)\n",
    "np.fill_diagonal(as_numpy, 0)\n",
    "np.savetxt('../data/graph_complete.csv', as_numpy, delimiter=',')\n",
    "graph_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce8b5a-7485-4b1f-9135-8b81315daa8e",
   "metadata": {},
   "source": [
    "Methods for initializing the dynamic simulation and creating the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52329600-96d4-41c3-b639-1f86db11f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_pubs(original_graph):\n",
    "    dates_list = [original_graph.nodes()[node]['date'] for node in original_graph.nodes()]\n",
    "    fs = []\n",
    "    print(min(dates_list).date())\n",
    "    for node in original_graph.nodes():\n",
    "        if original_graph.nodes()[node]['date'].date() == min(dates_list).date():\n",
    "            fs.append(original_graph.nodes()[node]['domain'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def create_i0(list_first_pubs, domains):\n",
    "    i0 = np.zeros(len(domains))\n",
    "    for pos, i in enumerate(i0):\n",
    "        if domains[pos] in list_first_pubs:\n",
    "            i0[pos] = 1\n",
    "            list_first_pubs.remove(domains[pos])\n",
    "\n",
    "    return i0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f3bbf-1d99-4d1c-91f8-35b2914a2228",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ba63f-3247-4ba5-858e-68a991a491b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_first_pubs = create_first_pubs(G)\n",
    "I0 = create_i0(list_first_pubs, all_nodes_domains)\n",
    "np.savetxt('../data/i0.csv', I0, delimiter=',')\n",
    "print('The number of inital infected is {}'.format(np.sum(I0)))\n",
    "I0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a92361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
