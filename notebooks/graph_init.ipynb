{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec5ae0",
   "metadata": {},
   "source": [
    "# Computing Graphs and initial conditions\n",
    "\n",
    "This notebook will instatiate and save the necessities for conducting the dynamical analysis.\n",
    "Firstly the dataset will be elaborated and then graphs and initial conditions will be computed.\n",
    "Might be necessary to change the data path (`PATH_TO_DATA`) or the dataset filnames (`WORD_VECTORS_FILENAME` and `ARTCLES_DF_FILENAME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747b823-cf56-4939-b9d5-8c86e64c94ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:30:02.566159Z",
     "start_time": "2022-01-08T16:30:02.205349Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "## These following have to be customized\n",
    "PATH_TO_DATA = Path('../data')\n",
    "# for joblib multithreading\n",
    "N_THREADS = -1\n",
    "# hyperparameters of the graph, get them from empirical data analysis\n",
    "MIN_SIM = 0.75\n",
    "TIME_THRESHOLD = 9*24 # in hours\n",
    "TIME_MIN = 4 # in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e96ab",
   "metadata": {},
   "source": [
    "These methods, used for building the graph, have been extracted and adapted from [this repo](https://github.com/elisamussumeci/modeling-news-spread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048cc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(\n",
    "    data, # DataFrame containing articles info\n",
    "    timestamp, # time of current article\n",
    "    similarities, # similarities of the current article\n",
    "    max_dt, # max dt (in hours) for two articles to be linked\n",
    "    min_dt, # min dt (in hours) for two articles to be linked\n",
    "    min_similarity, # minimum cos sim distance for two articles to be linked\n",
    "    outs, # current list of excluded articles\n",
    "    ):\n",
    "    while True:\n",
    "        # get the maximum similarity w.r.t. older articles\n",
    "        similarity = max(similarities)\n",
    "        # get the index of such max similar article\n",
    "        index = similarities.index(similarity)\n",
    "        # get dt in terms of hours\n",
    "        dt = (timestamp - data['timestamp'][index]).total_seconds() / 3600# / 24\n",
    "        # similarity threshold\n",
    "        if similarity < min_similarity:\n",
    "            # return None to add index to outs\n",
    "            return None\n",
    "        # continue if article is in outs or its distant in time\n",
    "        elif index in outs or dt > max_dt or dt <= min_dt:\n",
    "            similarities[index] = 0\n",
    "        # pass condition\n",
    "        else:\n",
    "            return index\n",
    "\n",
    "def create_graph(\n",
    "    dists_triu, # similarity matrix\n",
    "    data, # DataFrame containing articles info\n",
    "    time_max: int = 168, # max dt (in hours) for two articles to be linked\n",
    "    time_min: int = 6, # min dt (in hours) for two articles to be linked\n",
    "    sim_min: float = 0.8, # minimum cos sim distance for two articles to be linked\n",
    "    ):\n",
    "    # max number of nodes\n",
    "    n_articles = dists_triu.shape[0]\n",
    "    # instantiate the directed graph\n",
    "    G = nx.DiGraph()\n",
    "    # adding the first node\n",
    "    G.add_node(0, #step=0,\n",
    "               timestamp=data['timestamp'][0],\n",
    "               source=data['source'][0],\n",
    "               id=data['article_id'][0],\n",
    "               #children=[],\n",
    "              )\n",
    "    # instatiating elimination list\n",
    "    outs = []\n",
    "    # loop on the other articles\n",
    "    for i in range(1, n_articles):\n",
    "        # get time of current article\n",
    "        pub_i = data['timestamp'][i]\n",
    "        # get similarities\n",
    "        column = list(dists_triu[:, i])\n",
    "        # get index of an article related to the current one\n",
    "        index = get_index(data, pub_i, column, time_max, time_min, sim_min, outs)\n",
    "        # if a relation was found\n",
    "        if index != None:\n",
    "            # if the related article has not already been inserted, insert it\n",
    "            if index not in G.nodes():\n",
    "                G.add_nodes_from([(index, {\n",
    "                    'timestamp': data['timestamp'][index],\n",
    "                    'source': data['source'][index],\n",
    "                    'id': data['article_id'][index],\n",
    "                })])\n",
    "            # if the current article has not already been inserted, insert it\n",
    "            if i not in G.nodes():\n",
    "                G.add_nodes_from([(i, {\n",
    "                    'timestamp': data['timestamp'][i],\n",
    "                    'source': data['source'][i],\n",
    "                    'id': data['article_id'][i]\n",
    "                })])\n",
    "            # linking the nodes\n",
    "            G.add_edge(index, i)\n",
    "        # if a relation wa not found\n",
    "        else:\n",
    "            # add current article to elimination listS\n",
    "            outs.append(i)\n",
    "    # return the graph\n",
    "    return G\n",
    "\n",
    "# TODO: describe method, add comments\n",
    "def create_matrix_domain(graph):\n",
    "    \n",
    "    domain_list = []\n",
    "    for_domain = tqdm(\n",
    "        graph.nodes(),\n",
    "        leave=True,\n",
    "        unit='nodes_for_domain',\n",
    "    )\n",
    "    for pos in for_domain:\n",
    "        node = graph.nodes()._nodes[pos]\n",
    "        d = node['source']\n",
    "        if d not in domain_list:\n",
    "            domain_list.append(d)\n",
    "\n",
    "    df = pd.DataFrame(0, index = domain_list, columns = domain_list)\n",
    "    \n",
    "    for_nodes = tqdm(\n",
    "        graph.nodes(),\n",
    "        leave=True,\n",
    "        unit='nodes',\n",
    "    )\n",
    "    \n",
    "    for pos in for_nodes:\n",
    "        node = graph.nodes()._nodes[pos]\n",
    "        d = node['source']\n",
    "        successors = graph.successors(pos)\n",
    "        for suc in successors:\n",
    "            df[d][graph.nodes()._nodes[suc]['source']] += 1\n",
    "\n",
    "    return [domain_list, df]\n",
    "\n",
    "# TODO: describe method, add comments\n",
    "def create_complete_adjacency(graph, matrix):\n",
    "    \n",
    "    for_cols = tqdm(\n",
    "        [matrix[graph.nodes()._nodes[col]['source']] for col in graph.nodes()],\n",
    "        leave=True,\n",
    "        unit='nodes',\n",
    "    )\n",
    "    \n",
    "    def get_node_probs(domain_col_probs):\n",
    "        return np.array([\n",
    "            domain_col_probs[graph.nodes()._nodes[row]['source']] for row in graph.nodes()\n",
    "        ])\n",
    "    \n",
    "    probs = Parallel(n_jobs=N_THREADS)(delayed(get_node_probs)(i) for i in for_cols)\n",
    "    df = pd.DataFrame(probs, index=G.nodes(), columns=G.nodes())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30304027",
   "metadata": {},
   "source": [
    "Methods for initializing the dynamic simulation and creating the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_pubs(original_graph):\n",
    "    dates_list = [original_graph.nodes()._nodes[node]['timestamp'] for node in original_graph.nodes()]\n",
    "    fs = []\n",
    "    print(min(dates_list).date())\n",
    "    for node in original_graph.nodes():\n",
    "        if original_graph.nodes()._nodes[node]['timestamp'].date() == min(dates_list).date():\n",
    "            fs.append(original_graph.nodes()._nodes[node]['source'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def create_i0(list_first_pubs, domains):\n",
    "    i0 = np.zeros(len(domains))\n",
    "    for pos, i in enumerate(i0):\n",
    "        if domains[pos] in list_first_pubs:\n",
    "            i0[pos] = 1\n",
    "            list_first_pubs.remove(domains[pos])\n",
    "\n",
    "    return i0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11934b",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbc25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'dists_triu.csv',\n",
    "    'info_df.csv',\n",
    "    'empirical_graph.gpickle',\n",
    "    'empirical_graph_nodes.csv',\n",
    "    'graph_original_domains_each_node.txt',\n",
    "    'graph_complete.csv',\n",
    "    'i0.csv',\n",
    "    ]\n",
    "stories = [\n",
    "    'world_russia',\n",
    "    'world_norway',\n",
    "    'world_capitol_hill',\n",
    "]\n",
    "story_to_elaborate = 0\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[1]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    info_df = pd.read_csv(csv_file).drop(['Unnamed: 0'], 1)\n",
    "info_df = info_df.rename(columns={'id': 'article_id'})\n",
    "info_df['timestamp'] = pd.to_datetime(info_df.timestamp)\n",
    "print(info_df.head())\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[0]) as csv_file:\n",
    "    # dropping autospawned 'Unnamed: 0' column, and unecessary (since they are ordered already) 'article_id' column\n",
    "    dists_triu = pd.read_csv(csv_file, sep=',', header=None)\n",
    "dists_triu = dists_triu.values\n",
    "print(dists_triu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2772fc",
   "metadata": {},
   "source": [
    "Creating, saving and drawing the Graphs instatiated using _networkX_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f97f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph(dists_triu, info_df, time_max=TIME_THRESHOLD, time_min=TIME_MIN, sim_min=MIN_SIM)\n",
    "nx.write_gpickle(G, PATH_TO_DATA/stories[story_to_elaborate]/files[2])\n",
    "# nx.draw(G, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(G.nodes())).transpose().to_csv(PATH_TO_DATA/stories[story_to_elaborate]/files[3])\n",
    "all_nodes_domains = []\n",
    "for i in G.nodes():\n",
    "    all_nodes_domains.append(G.nodes()._nodes[i]['source'])\n",
    "\n",
    "with open(PATH_TO_DATA/stories[story_to_elaborate]/files[4], 'w') as file:\n",
    "    for item in all_nodes_domains:\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf33821-5ad5-4631-8f0d-00bbc1869e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_list, domain_matrix = create_matrix_domain(G)\n",
    "domain_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fed879-998d-427d-adbf-eb36e44ab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_complete = create_complete_adjacency(G, domain_matrix)\n",
    "graph_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96b530-6b18-4a9e-ae38-d470a72e58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_numpy = np.array(graph_complete)\n",
    "np.fill_diagonal(as_numpy, 0)\n",
    "np.savetxt(PATH_TO_DATA/stories[story_to_elaborate]/files[5], as_numpy, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f3bbf-1d99-4d1c-91f8-35b2914a2228",
   "metadata": {},
   "source": [
    "Retrieve and save initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ba63f-3247-4ba5-858e-68a991a491b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_first_pubs = create_first_pubs(G)\n",
    "print('Length of first day pubs is {}'.format(len(list_first_pubs)))\n",
    "I0 = create_i0(list_first_pubs, all_nodes_domains)\n",
    "np.savetxt(PATH_TO_DATA/stories[story_to_elaborate]/files[6], I0, delimiter=',')\n",
    "print('The number of inital infected is {}'.format(np.sum(I0)))\n",
    "print(I0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8828440-92d9-4152-ad38-2b72ddb414e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
