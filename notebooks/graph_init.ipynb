{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec5ae0",
   "metadata": {},
   "source": [
    "# Computing Graphs and initial conditions\n",
    "\n",
    "This notebook will instatiate and save the necessities for conducting the dynamical analysis.\n",
    "Firstly the dataset will be elaborated and then graphs and initial conditions will be computed.\n",
    "Might be necessary to change the data path (`PATH_TO_DATA`) or the dataset filnames (`WORD_VECTORS_FILENAME` and `ARTCLES_DF_FILENAME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5747b823-cf56-4939-b9d5-8c86e64c94ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:30:02.566159Z",
     "start_time": "2022-01-08T16:30:02.205349Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from gensim import corpora, models\n",
    "from gensim.models.word2vec import Word2Vec, KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "# import cugraph ## this library works with CUDA-capable GPUs but has many issues\n",
    "\n",
    "PATH_TO_DATA = Path('../data')\n",
    "WORD_VECTORS_FILENAME = 'words_dataframe.csv'\n",
    "ARTICLES_DF_FILENAME = 'info_dataframe.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49565de-1bd6-44ef-aae8-55ad2dedd36c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Similarity Matrix\n",
    "\n",
    "In order to create the **Similarity Matrix**, we should _load_ the 'WORD VECTORS' DataFrame and then we should _calculate the distances_ of all the articles (exploiting the simple dot product).\n",
    "\n",
    "The 'WORD VECTORS' must be normalized before computing the distance.\n",
    "The fastest method for normalization is that from _sklearn_, but also a _numpy_ version is provided (but not used).\n",
    "\n",
    "This is equivalent to use the _cosine similarity_ distance to compute these distances.\n",
    "\n",
    "This step should take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd44b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay</th>\n",
       "      <th>video</th>\n",
       "      <th>plaza</th>\n",
       "      <th>jan</th>\n",
       "      <th>juvenil</th>\n",
       "      <th>month</th>\n",
       "      <th>northeastern</th>\n",
       "      <th>nation</th>\n",
       "      <th>ireland</th>\n",
       "      <th>worri</th>\n",
       "      <th>...</th>\n",
       "      <th>amiri</th>\n",
       "      <th>flaccus</th>\n",
       "      <th>semi-retir</th>\n",
       "      <th>szelag</th>\n",
       "      <th>best-cas</th>\n",
       "      <th>cordoned-off</th>\n",
       "      <th>dearborn</th>\n",
       "      <th>blocked-off</th>\n",
       "      <th>cort</th>\n",
       "      <th>soul-search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 45484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     stay  video  plaza  jan  juvenil  month  northeastern  nation  ireland  \\\n",
       "0     0.0    4.0    0.0  0.0      0.0    2.0           0.0     7.0      0.0   \n",
       "1     0.0    0.0    0.0  0.0      0.0    0.0           0.0     1.0      0.0   \n",
       "2     0.0    0.0    0.0  0.0      0.0    0.0           0.0     0.0      0.0   \n",
       "3     0.0    0.0    0.0  1.0      0.0    0.0           0.0     1.0      0.0   \n",
       "4     0.0    7.0    0.0  0.0      0.0    0.0           0.0     3.0      0.0   \n",
       "..    ...    ...    ...  ...      ...    ...           ...     ...      ...   \n",
       "992   0.0    0.0    0.0  0.0      0.0    1.0           0.0     2.0      0.0   \n",
       "993   0.0    0.0    0.0  0.0      0.0    1.0           0.0     0.0      0.0   \n",
       "994   0.0    0.0    0.0  0.0      0.0    0.0           0.0     0.0      0.0   \n",
       "995   0.0    0.0    0.0  0.0      0.0    1.0           0.0     1.0      0.0   \n",
       "996   0.0    0.0    0.0  0.0      0.0    0.0           0.0     0.0      0.0   \n",
       "\n",
       "     worri  ...  amiri  flaccus  semi-retir  szelag  best-cas  cordoned-off  \\\n",
       "0      0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "1      0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "2      0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "3      0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "4      0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "..     ...  ...    ...      ...         ...     ...       ...           ...   \n",
       "992    0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "993    0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "994    0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "995    0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "996    0.0  ...    0.0      0.0         0.0     0.0       0.0           0.0   \n",
       "\n",
       "     dearborn  blocked-off  cort  soul-search  \n",
       "0         0.0          0.0   0.0          0.0  \n",
       "1         0.0          0.0   0.0          0.0  \n",
       "2         0.0          0.0   0.0          0.0  \n",
       "3         0.0          0.0   0.0          0.0  \n",
       "4         0.0          0.0   0.0          0.0  \n",
       "..        ...          ...   ...          ...  \n",
       "992       0.0          0.0   0.0          0.0  \n",
       "993       0.0          0.0   0.0          0.0  \n",
       "994       0.0          0.0   0.0          0.0  \n",
       "995       0.0          0.0   0.0          0.0  \n",
       "996       0.0          0.0   0.0          0.0  \n",
       "\n",
       "[997 rows x 45484 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "word_vectors = pd.read_csv(PATH_TO_DATA/WORD_VECTORS_FILENAME).drop(['Unnamed: 0','article_id'], 1)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9f4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_matrix(data, model):\n",
    "    \n",
    "    # get the list of words from data\n",
    "    texts = [list(map(lambda x: x.encode().decode(\"utf8\"), list(data.columns)))]\n",
    "\n",
    "    # corpora with all words from data, dictionary[id] returns the word linked to id\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    articles_iterator = tqdm(\n",
    "        range(len(data),\n",
    "        leave=True,\n",
    "        unit='columns',\n",
    "    )\n",
    "    \n",
    "    docvs = np.zeros((len(data), 300), dtype=float)\n",
    "    \n",
    "    # def process(article):\n",
    "    #     for word in list(data.columns):\n",
    "    #         weight = data[word][article]\n",
    "    #         w = dictionary.doc2idx([word])\n",
    "    #         try:\n",
    "    #             word_vector = model.word_vec(w[0])\n",
    "    #             try:\n",
    "    #                 assert np.isfinite(word_vector).all()\n",
    "    #             except AssertionError:\n",
    "    #                 print(w)\n",
    "    #         except KeyError:\n",
    "    #             word_vector = np.zeros((1, 300))\n",
    "    #     return docvs[article, :] + word_vector*weight\n",
    "            \n",
    "    # list_of_docvs = Parallel(n_jobs=12)(delayed(process)(i) for i in articles_iterator)\n",
    "    \n",
    "    for n in range(len(data)):\n",
    "        for word in list(data.columns):\n",
    "            weight = data[word][n]\n",
    "            w = dictionary.doc2idx([word])\n",
    "            try:\n",
    "                word_vector = model.word_vec(w[0])\n",
    "                try:\n",
    "                    assert np.isfinite(word_vector).all()\n",
    "                except AssertionError:\n",
    "                    print(w)\n",
    "            except KeyError:\n",
    "                word_vector = np.zeros((1, 300))\n",
    "\n",
    "            docvs[n, :] = docvs[n, :] + word_vector*weight\n",
    "    return docvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ca5e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7f090f3fb1f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(PATH_TO_DATA/'conceptnet-numberbatch-17-06-300.gz')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e17e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-28acb6c5dc87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_vector\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mlist_of_docvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn-env/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn-env/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the list of words from data\n",
    "texts = [list(map(lambda x: x.encode().decode(\"utf8\"), list(word_vectors.columns)))]\n",
    "\n",
    "# corpora with all words from data, dictionary[id] returns the word linked to id\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "articles_iterator = tqdm(\n",
    "    range(len(word_vectors)),\n",
    "    leave=True,\n",
    "    unit='articles',\n",
    ")\n",
    "\n",
    "def fn(article):\n",
    "    for word in list(word_vectors.columns):\n",
    "        weight = word_vectors[word][article]\n",
    "        w = dictionary.doc2idx([word])\n",
    "        try:\n",
    "            word_vector = model.word_vec(w[0])\n",
    "            try:\n",
    "                assert np.isfinite(word_vector).all()\n",
    "            except AssertionError:\n",
    "                print(w)\n",
    "        except KeyError:\n",
    "            word_vector = np.zeros((1, 300))\n",
    "    return word_vector*weight\n",
    "        \n",
    "list_of_docvs = Parallel(n_jobs=6)(delayed(fn)(i) for i in articles_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e96364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docvs = create_model_matrix(word_vectors, model)\n",
    "docvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8253e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative normalization procedure\n",
    "row_sums = docvs.sum(axis=1)\n",
    "np_docvs_norm = docvs / np.sqrt((row_sums**2).sum(-1))[:, np.newaxis]\n",
    "print(\"Shape of normalized matrix is {}.\".format(np_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(np_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(np_docvs_norm), np.min(np_docvs_norm)))\n",
    "np_docvs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b28e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn normalization procedure (axis=1)\n",
    "sk_docvs_norm = normalize(docvs)\n",
    "print(\"Shape of normalized matrix is {}.\".format(sk_docvs_norm.shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(sk_docvs_norm)))\n",
    "print(\"Max={}; Min={}.\".format(np.max(sk_docvs_norm), np.min(sk_docvs_norm)))\n",
    "sk_docvs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn normalization procedure (axis=0)\n",
    "sk_docvs_norm_0 = normalize(docvs)\n",
    "print(\"Shape of normalized matrix is {}.\".format(sk_docvs_norm._0shape))\n",
    "print(\"Sum of normalized matrix is {}.\".format(np.sum(sk_docvs_norm)_0))\n",
    "print(\"Max={}; Min={}.\".format(np.max(sk_docvs_norm)_0, np.min(sk_docvs_norm)_0))\n",
    "sk_docvs_norm_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712f297",
   "metadata": {},
   "source": [
    "Computation of the distance matrix.\n",
    "The simple dot product is used between the matrix and its transpose.\n",
    "Here are used `scipy.sparse` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee5180-2a9b-40bb-9105-843e378effe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sparse.csr_matrix(sk_docvs_norm)\n",
    "s_t = sparse.csr_matrix(sk_docvs_norm).T\n",
    "s_dist = s.dot(s_t)\n",
    "dists_triu = sparse.triu(s_dist, k=1)\n",
    "dists_triu = np.array(dists_triu.todense())\n",
    "np.savetxt('../data/dists_triu.csv', dists_triu, delimiter=',')\n",
    "dists_triu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40624608",
   "metadata": {},
   "source": [
    "These methods, used for building the graph, have been extracted and adapted from [this repo](https://github.com/elisamussumeci/modeling-news-spread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79ef81-343c-4ff8-abf8-44c3ebc17b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:32:36.476447Z",
     "start_time": "2022-01-08T16:32:36.427851Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pos(data, pub_i, column_list, time_max, sim_min, outs):\n",
    "    ans = False\n",
    "    pos = None\n",
    "\n",
    "    while ans is False:\n",
    "        sim = max(column_list)\n",
    "        pos = column_list.index(sim)\n",
    "        time_dif = (pub_i - data['timestamp'][pos]).total_seconds() / 3600\n",
    "        if sim < sim_min:\n",
    "            pos = None\n",
    "            ans = True\n",
    "        elif pos in outs or time_dif > time_max:\n",
    "            column_list[pos] = 0\n",
    "        else:\n",
    "            ans = True\n",
    "    return pos\n",
    "\n",
    "def create_graph(dists_triu, data, time_max=168, sim_min=0.8):\n",
    "    size = dists_triu.shape[0]\n",
    "    G = nx.DiGraph()\n",
    "    G.add_node(0, step=0, date=data['timestamp'][0], domain=data['source'][0], _id=data['article_id'][0],\n",
    "              children=[])\n",
    "    outs = []\n",
    "    for i in range(1,size):\n",
    "        pub_i = data['timestamp'][i]\n",
    "        column = list(dists_triu[:, i])\n",
    "        pos = get_pos(data, pub_i, column, time_max, sim_min, outs)\n",
    "\n",
    "        if pos != None:\n",
    "            if pos not in G.nodes():\n",
    "                domain_1 = data['source'][pos]\n",
    "                G.add_node(pos, date=data['timestamp'][pos], domain=domain_1,\n",
    "                           _id=data['article_id'][pos], children=[])\n",
    "            if i not in G.nodes():\n",
    "                domain_2 = data['source'][i]\n",
    "                G.add_node(i, date=pub_i, domain=domain_2, _id=data['article_id'][i], children=[])\n",
    "\n",
    "            G.add_edge(pos, i)\n",
    "        else:\n",
    "            outs.append(i)\n",
    "    return G\n",
    "\n",
    "def create_date(pub1, pub2, s):\n",
    "    dif = (pub2-pub1).total_seconds()/3600\n",
    "    return round((dif/s))\n",
    "\n",
    "def create_graphml(dists_triu, data, time_max=168, sim_min=0.8):\n",
    "    size = dists_triu.shape[0]\n",
    "    G = nx.DiGraph()\n",
    "    G.add_node(0, step=0, date=0, domain=data['source'][0])\n",
    "    date_init = data['timestamp'][0]\n",
    "    outs = []\n",
    "    for i in range(1, size):\n",
    "        pub_i = data['timestamp'][i]\n",
    "        column = list(dists_triu[:,i])\n",
    "        pos = get_pos(data, pub_i, column, time_max, sim_min, outs)\n",
    "\n",
    "        if pos != None:\n",
    "            if pos not in G.nodes():\n",
    "                domain_1 = data['source'][pos]\n",
    "                date_1 = create_date(date_init, data['timestamp'], 5)\n",
    "                G.add_node(pos, date=date_1, domain=domain_1)\n",
    "            if i not in G.nodes():\n",
    "                domain_2 = data['source'][i]\n",
    "                date_2 = create_date(date_init, pub_i, 5)\n",
    "                G.add_node(i, date=date_2, domain=domain_2)\n",
    "\n",
    "            G.add_edge(pos, i)\n",
    "        else:\n",
    "            outs.append(i)\n",
    "    return G\n",
    "\n",
    "def create_matrix_domain(graph):\n",
    "    \n",
    "    domain_list = []\n",
    "    for pos in graph.nodes():\n",
    "        node = graph.nodes()[pos]\n",
    "        d = node['domain']\n",
    "        if d not in domain_list:\n",
    "            domain_list.append(d)\n",
    "\n",
    "    df = pd.DataFrame(0, index = domain_list, columns = domain_list)\n",
    "\n",
    "    for pos in graph.nodes():\n",
    "        node = graph.nodes()[pos]\n",
    "        d = node['domain']\n",
    "        successors = graph.successors(pos)\n",
    "        for suc in successors:\n",
    "            df[d][graph.nodes()[suc]['domain']] += 1\n",
    "\n",
    "    return [domain_list, df]\n",
    "\n",
    "def create_complete_adjacency(graph, matrix):\n",
    "    df = pd.DataFrame(0, index=graph.nodes(), columns=graph.nodes())\n",
    "    for column in graph.nodes():\n",
    "        i_domains_column = matrix[graph.nodes()[column]['domain']]\n",
    "        for row in graph.nodes():\n",
    "            prob = i_domains_column[graph.nodes()[row]['domain']]\n",
    "            df[column][row] = prob\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2772fc",
   "metadata": {},
   "source": [
    "Creating, saving and drawing the Graphs instatiated using _networkX_ library.\n",
    "This step should take ~6min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06e015-eb6f-4088-9e70-b83ace836917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:32:39.201018Z",
     "start_time": "2022-01-08T16:32:39.189349Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv(PATH_TO_DATA/ARTICLES_DF_FILENAME)\n",
    "articles['timestamp'] = pd.to_datetime(articles.timestamp)\n",
    "articles = articles.drop('Unnamed: 0', 1)\n",
    "articles = articles.rename(columns={'id': 'article_id'})\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f97f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph(dists_triu, articles)\n",
    "nx.write_gpickle(G, '../data/empirical_graph.gpickle')\n",
    "# nx.draw(G, with_labels=True, font_weight='bold')\n",
    "# plt.show()\n",
    "H = create_graphml(dists_triu, articles)\n",
    "nx.write_graphml(H, '../data/empirical_graph.graphml')\n",
    "# nx.draw(H, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(G.nodes())).transpose().to_csv('../data/empirical_graph_nodes.csv')\n",
    "all_nodes_domains = []\n",
    "for i in G.nodes():\n",
    "    all_nodes_domains.append(G.nodes()[i]['domain'])\n",
    "\n",
    "f = open('../data/graph_original_domains_each_node.txt', 'w')\n",
    "for item in all_nodes_domains:\n",
    "    f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa14860",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_list, domain_matrix = create_matrix_domain(G)\n",
    "graph_complete = create_complete_adjacency(G, domain_matrix)\n",
    "as_numpy = np.array(graph_complete)\n",
    "np.fill_diagonal(as_numpy, 0)\n",
    "np.savetxt('../data/graph_complete.csv', as_numpy, delimiter=',')\n",
    "graph_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce8b5a-7485-4b1f-9135-8b81315daa8e",
   "metadata": {},
   "source": [
    "Methods for initializing the dynamic simulation and creating the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52329600-96d4-41c3-b639-1f86db11f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_pubs(original_graph):\n",
    "    dates_list = [original_graph.nodes()[node]['date'] for node in original_graph.nodes()]\n",
    "    fs = []\n",
    "    print(min(dates_list).date())\n",
    "    for node in original_graph.nodes():\n",
    "        if original_graph.nodes()[node]['date'].date() == min(dates_list).date():\n",
    "            fs.append(original_graph.nodes()[node]['domain'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def create_i0(list_first_pubs, domains):\n",
    "    i0 = np.zeros(len(domains))\n",
    "    for pos, i in enumerate(i0):\n",
    "        if domains[pos] in list_first_pubs:\n",
    "            i0[pos] = 1\n",
    "            list_first_pubs.remove(domains[pos])\n",
    "\n",
    "    return i0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f3bbf-1d99-4d1c-91f8-35b2914a2228",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ba63f-3247-4ba5-858e-68a991a491b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_first_pubs = create_first_pubs(G)\n",
    "I0 = create_i0(list_first_pubs, all_nodes_domains)\n",
    "np.savetxt('../data/i0.csv', I0, delimiter=',')\n",
    "print('The number of inital infected is {}'.format(np.sum(I0)))\n",
    "I0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a92361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
